{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: Import LlamaIndex / Vertex AI integration*"
      ],
      "metadata": {
        "id": "bgLvGkhZ5Vwa"
      },
      "id": "bgLvGkhZ5Vwa"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet llama-index-llms-vertex"
      ],
      "metadata": {
        "id": "g6jjydfv5NBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fca2940-697b-4b04-88b7-9d322295700e"
      },
      "id": "g6jjydfv5NBq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d86f6cd",
      "metadata": {
        "id": "3d86f6cd"
      },
      "source": [
        "# Document Search with LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4168e6b6",
      "metadata": {
        "id": "4168e6b6"
      },
      "source": [
        "This example shows how to use the Python [LlamaIndex](https://docs.llamaindex.ai/en/stable/) library to run a text-generation request on open-source LLMs and embedding models using the OpenAI SDK, then augment that request using the text stored in a collection of local PDF documents.\n",
        "\n",
        "### <u>Requirements</u>\n",
        "1. As you will accessing the LLMs and embedding models through Vector AI Engineering's Kaleidoscope Service (Vector Inference + Autoscaling), you will need to request a KScope API Key:\n",
        "\n",
        "      Run the following command (replace ```<user_id>``` and ```<password>```) from **within the cluster** to obtain the API Key. The ```access_token``` in the output is your KScope API Key.\n",
        "  ```bash\n",
        "  curl -X POST -d \"grant_type=password\" -d \"username=<user_id>\" -d \"password=<password>\" https://kscope.vectorinstitute.ai/token\n",
        "  ```\n",
        "2. After obtaining the `.env` configurations, make sure to create the ```.kscope.env``` file in your home directory (```/h/<user_id>```) and set the following env variables:\n",
        "- For local models through Kaleidoscope (KScope):\n",
        "    ```bash\n",
        "    export OPENAI_BASE_URL=\"https://kscope.vectorinstitute.ai/v1\"\n",
        "    export OPENAI_API_KEY=<kscope_api_key>\n",
        "    ```\n",
        "- For OpenAI models:\n",
        "   ```bash\n",
        "   export OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n",
        "   export OPENAI_API_KEY=<openai_api_key>\n",
        "   ```\n",
        "3. (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e4da1f",
      "metadata": {
        "id": "22e4da1f"
      },
      "source": [
        "## Set up the RAG workflow environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "965ad1b0-76d3-4db8-9705-a4a4ac56ebca",
      "metadata": {
        "id": "965ad1b0-76d3-4db8-9705-a4a4ac56ebca"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a750497-5fc0-4a7f-8ed6-931c5d8759d4",
      "metadata": {
        "id": "4a750497-5fc0-4a7f-8ed6-931c5d8759d4"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: added the below in colab enterprise - install faiss and langchain dependencies*"
      ],
      "metadata": {
        "id": "l4BUoOWN7FUy"
      },
      "id": "l4BUoOWN7FUy"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet faiss-cpu langchain llama-index-vector-stores-faiss"
      ],
      "metadata": {
        "id": "UKiMOf6R6uz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6223d862-1d24-4430-db18-6b42792f1182"
      },
      "id": "UKiMOf6R6uz2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f637730",
      "metadata": {
        "id": "2f637730"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.node_parser import LangchainNodeParser\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# jkwng: commented out the following on Vertex AI - faiss is in memory\n",
        "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "# from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97d9175-73d5-4ef0-965b-acaa3fb4a91c",
      "metadata": {
        "id": "b97d9175-73d5-4ef0-965b-acaa3fb4a91c"
      },
      "source": [
        "#### Load config files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96933bd8-4158-4c50-ad05-2a134d490fdc",
      "metadata": {
        "id": "96933bd8-4158-4c50-ad05-2a134d490fdc"
      },
      "outputs": [],
      "source": [
        "# Add root folder of the rag_bootcamp repo to PYTHONPATH\n",
        "current_dir = Path().resolve()\n",
        "parent_dir = current_dir.parent\n",
        "sys.path.insert(0, str(parent_dir))\n",
        "\n",
        "#jkwng: we don't need this ?\n",
        "# from utils.load_secrets import load_env_file\n",
        "# load_env_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be12a21-c830-4aa3-a76d-3684b9445950",
      "metadata": {
        "id": "4be12a21-c830-4aa3-a76d-3684b9445950"
      },
      "outputs": [],
      "source": [
        "\n",
        "#jkwng: we don't need this?\n",
        "# GENERATOR_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12ecf9ac",
      "metadata": {
        "id": "12ecf9ac"
      },
      "source": [
        "#### Set up some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4e2417",
      "metadata": {
        "id": "dd4e2417"
      },
      "outputs": [],
      "source": [
        "def pretty_print_docs(docs):\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"Document {i+1}:\\n\\n\" + d.text for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9edd103",
      "metadata": {
        "id": "d9edd103"
      },
      "source": [
        "#### Make sure other necessary items are in place"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: put the input files on GCS, here we test to make sure we can read our dataset*"
      ],
      "metadata": {
        "id": "BcuTfd5u03h8"
      },
      "id": "BcuTfd5u03h8"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "bucket_name = \"jkwng-vertex-experiments\"\n",
        "prefix = \"rag_bootcamp/document_search/source_documents\"\n",
        "\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "bloblist = bucket.list_blobs(prefix=prefix)\n",
        "\n",
        "for blob in bloblist:\n",
        "    print(blob.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZlcdJrC88Ge",
        "outputId": "84388f5d-999e-4605-d08e-95401156a1a1"
      },
      "id": "uZlcdJrC88Ge",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rag_bootcamp/document_search/source_documents/vector-institute-2021-22-annual-report_accessible.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b61e4f",
      "metadata": {
        "id": "74b61e4f"
      },
      "outputs": [],
      "source": [
        "# Look for the source_documents folder and make sure there is at least 1 pdf file here\n",
        "contains_pdf = False\n",
        "\n",
        "#jkwng: migrated this to GCS\n",
        "\n",
        "#directory_path = \"./source_documents\"\n",
        "# if not os.path.exists(directory_path):\n",
        "    # print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
        "# for filename in os.listdir(directory_path):\n",
        "bloblist = bucket.list_blobs(prefix=prefix)\n",
        "for blob in bloblist:\n",
        "    contains_pdf = True if \".pdf\" in blob.name else contains_pdf\n",
        "if not contains_pdf:\n",
        "    print(f\"ERROR: The gs://{bucket_name}/{prefix} subfolder must contain at least one .pdf file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c3d1c8-07cb-4e1a-88ac-087536c6e96e",
      "metadata": {
        "id": "52c3d1c8-07cb-4e1a-88ac-087536c6e96e"
      },
      "source": [
        "#### Choose LLM and embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: use Gemini 2.0 Flash and Gemini text embedding models instead of Llama and BGE - we can also deploy these models via Vertex model garden*"
      ],
      "metadata": {
        "id": "7hdskcYT1EZa"
      },
      "id": "7hdskcYT1EZa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a10552-cb1a-4088-9081-05494fca9410",
      "metadata": {
        "id": "78a10552-cb1a-4088-9081-05494fca9410"
      },
      "outputs": [],
      "source": [
        "# GENERATOR_MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct\"\n",
        "# EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\n",
        "\n",
        "GENERATOR_MODEL_NAME = \"gemini-2.0-flash-001\"\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-005\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e558afb",
      "metadata": {
        "id": "1e558afb"
      },
      "source": [
        "## Start with a basic generation request without RAG augmentation\n",
        "\n",
        "Let's start by asking Llama-3.1 a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's world knowledge that we expect the LLM to know.\n",
        "\n",
        "Instead, we want to ask it a question that is domain-specific and it won't know the answer to. A good example would be an obscure detail buried deep within a company's annual report. For example:\n",
        "\n",
        "*How many Vector scholarships in AI were awarded in 2022?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6133a928",
      "metadata": {
        "id": "6133a928"
      },
      "outputs": [],
      "source": [
        "query = \"How many Vector scholarships in AI were awarded in 2022?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358a22c5",
      "metadata": {
        "id": "358a22c5"
      },
      "source": [
        "## Now send the query to the open source model using KScope"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: send the generation to Gemini 2.0 Flash*"
      ],
      "metadata": {
        "id": "34C7_NiD1Ttz"
      },
      "id": "34C7_NiD1Ttz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3d559a-74cf-4406-9ee4-61944f3e4b65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f3d559a-74cf-4406-9ee4-61944f3e4b65",
        "outputId": "041967db-9438-4f77-ecfa-bedd0cd97697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: \n",
            "\n",
            "assistant: According to the Vector Institute, they awarded **170** Vector Scholarships in Artificial Intelligence in 2022.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.vertex import Vertex\n",
        "\n",
        "# llm = OpenAILike(\n",
        "#     model=GENERATOR_MODEL_NAME,\n",
        "#     is_chat_model=True,\n",
        "#     temperature=0,\n",
        "#     max_tokens=None,\n",
        "#     api_base=GENERATOR_BASE_URL,\n",
        "#     api_key=OPENAI_API_KEY\n",
        "# )\n",
        "\n",
        "#jkwng: send to gemini 2.0\n",
        "llm = Vertex(\n",
        "    model=GENERATOR_MODEL_NAME,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "message = [\n",
        "    ChatMessage(\n",
        "        role=\"user\",\n",
        "        content=query\n",
        "    )\n",
        "]\n",
        "try:\n",
        "    result = llm.chat(message)\n",
        "    print(f\"Result: \\n\\n{result}\")\n",
        "except Exception as err:\n",
        "    if \"Error code: 503\" in err.message:\n",
        "        print(f\"The model {GENERATOR_MODEL_NAME} is not ready yet.\")\n",
        "    else:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e1c200",
      "metadata": {
        "id": "e6e1c200"
      },
      "source": [
        "Without additional information, Llama-3.1 is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0255ea68",
      "metadata": {
        "id": "0255ea68"
      },
      "source": [
        "## Ingestion: Load and store the documents from `source_documents`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9d0304",
      "metadata": {
        "id": "ba9d0304"
      },
      "source": [
        "Start by reading in all the PDF files from `source_documents`, break them up into smaller digestible chunks, then encode them as vector embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng - add the llama gcs integration*"
      ],
      "metadata": {
        "id": "tUVx-mI22fg0"
      },
      "id": "tUVx-mI22fg0"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet llama-index-readers-gcs"
      ],
      "metadata": {
        "id": "E3wd_DCq_vRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb4ca09-ae47-4a9b-c5f7-9639d32628d7"
      },
      "id": "E3wd_DCq_vRC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: we use the LlamaIndex GCS integration to read the input files directly off of GCS*"
      ],
      "metadata": {
        "id": "AAkuxXRx2OPj"
      },
      "id": "AAkuxXRx2OPj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5710c72d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5710c72d",
        "outputId": "2ea4fbef-7012-4b43-ca09-f863ce1119a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.readers.gcs.base:No explicit credentials provided. Falling back to default credentials.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of source documents: 42\n",
            "Number of text chunks: 196\n"
          ]
        }
      ],
      "source": [
        "from llama_index.readers.gcs import GCSReader\n",
        "\n",
        "# Load the pdfs\n",
        "docs = GCSReader(bucket=bucket_name, prefix=prefix).load_data()\n",
        "\n",
        "# directory_path = \"./source_documents\"\n",
        "# os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "print(f\"Number of source documents: {len(docs)}\")\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "parser = LangchainNodeParser(RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32))\n",
        "chunks = parser.get_nodes_from_documents(docs)\n",
        "print(f\"Number of text chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a7545e",
      "metadata": {
        "id": "b4a7545e"
      },
      "source": [
        "#### Define the embeddings model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: use gemini embeddings model*"
      ],
      "metadata": {
        "id": "LRWzioSHX0O_"
      },
      "id": "LRWzioSHX0O_"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet llama-index-embeddings-vertex"
      ],
      "metadata": {
        "id": "7gJp7AxYXwzo"
      },
      "id": "7gJp7AxYXwzo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268ab345-4676-4700-8965-4639751e7fe8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "268ab345-4676-4700-8965-4639751e7fe8",
        "outputId": "ffeb1ee0-4af5-4226-c767-b1c75a6d1140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up the embeddings model...\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
        "import google.auth\n",
        "\n",
        "credentials, project_id = google.auth.default()\n",
        "\n",
        "print(f\"Setting up the embeddings model...\")\n",
        "# embeddings = HuggingFaceEmbedding(\n",
        "#     model_name=EMBEDDING_MODEL_NAME,\n",
        "#     device='cuda',\n",
        "#     trust_remote_code=True,\n",
        "# )\n",
        "\n",
        "embeddings = VertexTextEmbedding(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    credentials=credentials,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7ed121-6e4c-46e4-926c-33aa6ee77759",
      "metadata": {
        "id": "ee7ed121-6e4c-46e4-926c-33aa6ee77759"
      },
      "source": [
        "#### Set LLM and embedding model [recommended for LlamaIndex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7446327c-d8b9-4928-92c7-fb0af4fb0fdc",
      "metadata": {
        "id": "7446327c-d8b9-4928-92c7-fb0af4fb0fdc"
      },
      "outputs": [],
      "source": [
        "Settings.llm = llm\n",
        "Settings.embed_model = embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5050576-073c-4615-9621-b6b217a13b0e",
      "metadata": {
        "id": "f5050576-073c-4615-9621-b6b217a13b0e"
      },
      "source": [
        "## Retrieval: Make the document chunks available via a retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0131a2d-4cd6-4c1e-835e-540329fda2b2",
      "metadata": {
        "id": "d0131a2d-4cd6-4c1e-835e-540329fda2b2"
      },
      "source": [
        "The retriever will identify the document chunks that most closely match our original query. (This takes about 1-2 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49d0093-0105-499a-a7e3-ebf6326a85d9",
      "metadata": {
        "id": "c49d0093-0105-499a-a7e3-ebf6326a85d9"
      },
      "outputs": [],
      "source": [
        "def get_embed_model_dim(embed_model):\n",
        "    embed_out = embed_model.get_text_embedding(\"Dummy Text\")\n",
        "    return len(embed_out)\n",
        "\n",
        "faiss_dim = get_embed_model_dim(embeddings)\n",
        "faiss_index = faiss.IndexFlatL2(faiss_dim)\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex(chunks, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f512cb-36f8-4afb-a8c6-0c187a0d9cae",
      "metadata": {
        "id": "37f512cb-36f8-4afb-a8c6-0c187a0d9cae"
      },
      "outputs": [],
      "source": [
        "retriever = index.as_retriever(similarity_top_k=5)\n",
        "\n",
        "# Retrieve the most relevant context from the vector store based on the query\n",
        "retrieved_docs = retriever.retrieve(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68093b7f-4da7-45d8-8a58-536fb7f8aa5c",
      "metadata": {
        "id": "68093b7f-4da7-45d8-8a58-536fb7f8aa5c"
      },
      "source": [
        "Let's see what results it found. Important to note, these results are in the order the retriever thought were the best matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ff6d3c-b6e8-4702-8591-44e0d7b7d484",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ff6d3c-b6e8-4702-8591-44e0d7b7d484",
        "outputId": "3e14f04f-8b09-4c3a-8a63-c4e286103e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "\n",
            "26 \n",
            " \n",
            " \n",
            "VECTOR SCHOLARSHIPS IN \n",
            "AI ATTRACT TOP TALENT \n",
            "TO ONTARIO UNIVERSITIES \n",
            "109 \n",
            "Vector Scholarships in AI awarded \n",
            "34 \n",
            "Programs \n",
            "13 \n",
            "Universities \n",
            "351 \n",
            "Scholarships awarded since the \n",
            "program launched in 2018 \n",
            "Supported with funding from the Province of \n",
            "Ontario, the Vector Institute Scholarship in Artifcial \n",
            "Intelligence (VSAI) helps Ontario universities to attract \n",
            "the best and brightest students to study in AI-related \n",
            "master’s programs. \n",
            "Scholarship recipients connect directly with leading\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 2:\n",
            "\n",
            "5 \n",
            "Annual Report 2021–22Vector Institute\n",
            "SPOTLIGHT ON FIVE YEARS OF AI \n",
            "LEADERSHIP FOR CANADIANS \n",
            "SINCE THE VECTOR INSTITUTE WAS FOUNDED IN 2017: \n",
            "2,080+ \n",
            "Students have graduated from \n",
            "Vector-recognized AI programs and \n",
            "study paths \n",
            "$6.2 M \n",
            "Scholarship funds committed to \n",
            "students in AI programs \n",
            "3,700+ \n",
            "Postings for AI-focused jobs and \n",
            "internships ofered on Vector’s \n",
            "Digital Talent Hub \n",
            "$103 M \n",
            "In research funding committed to \n",
            "Vector-afliated researchers \n",
            "94 \n",
            "Research awards earned by\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 3:\n",
            "\n",
            "94 \n",
            "Research awards earned by  \n",
            "Vector Faculty Members \n",
            "470+ \n",
            "people from 35 industry and health \n",
            "sector organizations involved in 13 \n",
            "completed collaborative projects \n",
            "~20 \n",
            "Thought-leadership articles \n",
            "published on important topics in AI \n",
            "35+ \n",
            "partnerships and agreements with \n",
            "leading health sector organizations\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 4:\n",
            "\n",
            "mock technical interviews \n",
            "43 Applied interns in Vector’s \n",
            "expanded internship program \n",
            "(up from 5 last year), working on \n",
            "Vector projects across its industry \n",
            "innovation, health, research, and \n",
            "AI engineering teams\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Document 5:\n",
            "\n",
            "23 \n",
            "RESEARCH AWARDS AND \n",
            "ACHIEVEMENTS \n",
            "Each year, members of Vector’s research community \n",
            "are recognized for outstanding contributions to AI and \n",
            "machine learning felds. Highlights of 2021–22 include: \n",
            "GLOBAL REACH OF VECTOR \n",
            "RESEARCHERS AND THEIR WORK \n",
            "Vector researchers published papers, gave \n",
            "presentations, or led workshops at many of the \n",
            "top AI conferences this year, including NeurIPS, \n",
            "CVPR, ICLR, ICML, and ACM FAccT. \n",
            "380+ Research papers presented at  \n",
            "high-impact global \n",
            "conferences and in top-\n"
          ]
        }
      ],
      "source": [
        "pretty_print_docs(retrieved_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3008507b",
      "metadata": {
        "id": "3008507b"
      },
      "source": [
        "## Now send the query to the RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23499f4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23499f4a",
        "outputId": "0078b138-36e0-481c-df96-863553041d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: \n",
            "\n",
            "In 2022, 109 Vector Scholarships in AI were awarded. Since the program's launch in 2018, a total of 351 scholarships have been awarded.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query_engine = RetrieverQueryEngine(retriever=retriever)\n",
        "result = query_engine.query(query)\n",
        "print(f\"Result: \\n\\n{result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb632b45-b135-4561-9759-99fcc03e6959",
      "metadata": {
        "id": "cb632b45-b135-4561-9759-99fcc03e6959"
      },
      "source": [
        "The model provides the correct answer (109) using the retrieved information."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag_dataloaders",
      "language": "python",
      "name": "rag_dataloaders"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "name": "darmod-document_search_llamaindex_vertexai.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}