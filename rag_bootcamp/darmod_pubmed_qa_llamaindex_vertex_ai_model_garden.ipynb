{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e979eb9-3fe0-49c2-b814-f4025ed81934",
      "metadata": {
        "id": "2e979eb9-3fe0-49c2-b814-f4025ed81934"
      },
      "source": [
        "# PubMed QA using LlamaIndex\n",
        "\n",
        "## Introduction\n",
        "This notebook presents a RAG workflow for the [PubMed QA](https://pubmedqa.github.io/) task using [LlamaIndex](https://www.llamaindex.ai/). The code is written in a configurable fashion, giving you the flexibility to edit the RAG configuration and observe the change in output/responses.\n",
        "\n",
        "It covers a step-by-step procedure for building the RAG workflow (Stages 1-4) and later runs the pipeline on a sample from the dataset. The notebook also covers the sparse, dense, hybrid retrieval strategies along with the re-ranker. We have alse added an optional component for RAG evaluation using the [Ragas](https://docs.ragas.io/en/stable/) library.\n",
        "\n",
        "### <u>Requirements</u>\n",
        "1. As you will accessing the LLMs and embedding models through Vector AI Engineering's Kaleidoscope Service (Vector Inference + Autoscaling), you will need to request a KScope API Key:\n",
        "\n",
        "      Run the following command (replace ```<user_id>``` and ```<password>```) from **within the cluster** to obtain the API Key. The ```access_token``` in the output is your KScope API Key.\n",
        "  ```bash\n",
        "  curl -X POST -d \"grant_type=password\" -d \"username=<user_id>\" -d \"password=<password>\" https://kscope.vectorinstitute.ai/token\n",
        "  ```\n",
        "2. After obtaining the `.env` configurations, make sure to create the ```.kscope.env``` file in your home directory (```/h/<user_id>```) and set the following env variables:\n",
        "- For local models through Kaleidoscope (KScope):\n",
        "    ```bash\n",
        "    export OPENAI_BASE_URL=\"https://kscope.vectorinstitute.ai/v1\"\n",
        "    export OPENAI_API_KEY=<kscope_api_key>\n",
        "    ```\n",
        "- For OpenAI models:\n",
        "   ```bash\n",
        "   export OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n",
        "   export OPENAI_API_KEY=<openai_api_key>\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e83bf1-b093-4acd-ad37-27bd56f5b8e4",
      "metadata": {
        "id": "d6e83bf1-b093-4acd-ad37-27bd56f5b8e4"
      },
      "source": [
        "## STAGE 0 - Set up the RAG workflow environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4e45a6-1ed6-4e90-b0b7-913566652b40",
      "metadata": {
        "id": "2d4e45a6-1ed6-4e90-b0b7-913566652b40"
      },
      "source": [
        "#### Import libraries, custom classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet \\\n",
        "  llama-index \\\n",
        "  google-cloud-secret-manager \\\n",
        "  datasets \\\n",
        "  llama-index-readers-json \\\n",
        "  llama-index-readers-file \\\n",
        "  llama-index-readers-gcs \\\n",
        "  llama-index-embeddings-vertex \\\n",
        "  llama-index-embeddings-google-genai \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-embeddings-text-embeddings-inference \\\n",
        "  llama-index-embeddings-vertex-endpoint \\\n",
        "  llama-index-llms-huggingface \\\n",
        "  llama-index-llms-openai-like \\\n",
        "  llama-index-llms-vertex \\\n",
        "  faiss-cpu \\\n",
        "  llama-index-vector-stores-faiss \\\n",
        "  llama-index-vector-stores-weaviate \\\n",
        "  llama-index-vector-stores-vertexaivectorsearch \\\n",
        "  llama-index-retrievers-bm25 \\\n",
        "  rapidfuzz \\\n",
        "  ragas \\\n",
        "  pydantic>=2.10.4 \\\n",
        "  google-cloud-aiplatform>=1.76 \\\n",
        "  langchain-core \\\n",
        "  langchain-cohere \\\n",
        "  langchain-huggingface \\\n",
        "  langchain-google-vertexai\n",
        "# jkwng: restart the kernel after this"
      ],
      "metadata": {
        "id": "-qhpaWUvWKzA"
      },
      "id": "-qhpaWUvWKzA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1909a34-2063-4195-a90e-fd35ce1b5df5",
      "metadata": {
        "id": "c1909a34-2063-4195-a90e-fd35ce1b5df5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82038178-2699-403b-b32a-342b8d19ae98",
      "metadata": {
        "id": "82038178-2699-403b-b32a-342b8d19ae98"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "from llama_index.core import ServiceContext, Settings, set_global_handler\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "\n",
        "# jkwng: in order to make this notebook self contained, i just cut and paste these into the notebook\n",
        "# from task_dataset import PubMedQATaskDataset\n",
        "\n",
        "# from utils.hosting_utils import RAGLLM\n",
        "# from utils.rag_utils import (\n",
        "#     DocumentReader, RAGEmbedding, RAGQueryEngine, RagasEval,\n",
        "#     extract_yes_no, validate_rag_cfg\n",
        "#     )\n",
        "# from utils.storage_utils import RAGIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23409b3f-d436-469f-90eb-6ff4a7ce9c92",
      "metadata": {
        "id": "23409b3f-d436-469f-90eb-6ff4a7ce9c92"
      },
      "source": [
        "#### Load config files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: we don't need this cell*"
      ],
      "metadata": {
        "id": "qoKgmcb64nPK"
      },
      "id": "qoKgmcb64nPK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94487a03-5c16-4683-b10b-026171b8b37d",
      "metadata": {
        "id": "94487a03-5c16-4683-b10b-026171b8b37d"
      },
      "outputs": [],
      "source": [
        "# Add root folder of the rag_bootcamp repo to PYTHONPATH\n",
        "current_dir = Path().resolve()\n",
        "parent_dir = current_dir.parent\n",
        "sys.path.insert(0, str(parent_dir))\n",
        "\n",
        "\n",
        "# from utils.load_secrets import load_env_file\n",
        "# load_env_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: or this cell*"
      ],
      "metadata": {
        "id": "7RZnoMQF4toP"
      },
      "id": "7RZnoMQF4toP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a2b082-8f20-4567-9e37-69f0beaee9ef",
      "metadata": {
        "id": "24a2b082-8f20-4567-9e37-69f0beaee9ef"
      },
      "outputs": [],
      "source": [
        "# GENERATOR_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: however we need these variables specifically for deployment on Google Cloud*"
      ],
      "metadata": {
        "id": "WVABlJxFaYBL"
      },
      "id": "WVABlJxFaYBL"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "GCS_URI = \"jkwng-vertex-experiments/rag_bootcamp/pubmed_qa\""
      ],
      "metadata": {
        "id": "kio2HT3-abCx"
      },
      "id": "kio2HT3-abCx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c70d889-6e10-45b7-be3b-52a5b56d4f43",
      "metadata": {
        "id": "8c70d889-6e10-45b7-be3b-52a5b56d4f43"
      },
      "source": [
        "#### Set RAG configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below: using `bge-base-en-v1.5` for embeddings and Llama 3.1 8B instruct for generation, both hosted on Vertex Endpoints. Use Gemini 2.0 Flash for LLM-based evals"
      ],
      "metadata": {
        "id": "IXPSTLTwiAkr"
      },
      "id": "IXPSTLTwiAkr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab816260-2e3f-4ac3-a3f7-623202b116bd",
      "metadata": {
        "id": "ab816260-2e3f-4ac3-a3f7-623202b116bd"
      },
      "outputs": [],
      "source": [
        "rag_cfg = {\n",
        "    # Node parser config\n",
        "    \"chunk_size\": 256,\n",
        "    \"chunk_overlap\": 0,\n",
        "\n",
        "    # Embedding model config\n",
        "    # \"embed_model_type\": \"hf\",\n",
        "    # \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n",
        "    \"embed_model_type\": \"vertex-endpoint\",\n",
        "    \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n",
        "    \"embed_model_endpoint_id\": \"83814671873736704\", # endpoint id\n",
        "    \"embed_model_use_dedicated_endpoint\": True,\n",
        "    \"embed_model_dedicated_dns\": \"83814671873736704.us-central1-205512073711.prediction.vertexai.goog\",\n",
        "\n",
        "    # LLM config\n",
        "    # \"llm_type\": \"kscope\",\n",
        "    # \"llm_name\": \"Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"llm_type\": \"vertex-endpoint\",\n",
        "    \"llm_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"llm_endpoint_id\": \"133354267774812160\",\n",
        "    \"llm_use_dedicated_endpoint\": True,\n",
        "    \"llm_dedicated_dns\": \"133354267774812160.us-central1-205512073711.prediction.vertexai.goog\",\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"temperature\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 50,\n",
        "    \"do_sample\": False,\n",
        "\n",
        "    # Vector DB config\n",
        "    \"vector_db_type\": \"weaviate\", # \"weaviate\"\n",
        "    # \"vector_db_type\": \"vertex\",\n",
        "    \"vector_db_name\": \"Pubmed_QA\",\n",
        "    # MODIFY THIS\n",
        "    \"weaviate_url\": \"https://ds4tx7ttr3ciaui5obmowg.c0.us-east1.gcp.weaviate.cloud\",\n",
        "\n",
        "    # Retriever and query config\n",
        "    \"retriever_type\": \"vector_index\", # \"vector_index\"\n",
        "    \"retriever_similarity_top_k\": 5,\n",
        "    \"query_mode\": \"default\", # \"default\", \"hybrid\" - jkwng: changed to default\n",
        "    \"hybrid_search_alpha\": 0.0, # float from 0.0 (sparse search - bm25) to 1.0 (vector search)\n",
        "    \"response_mode\": \"compact\",\n",
        "    \"use_reranker\": False,\n",
        "    \"rerank_top_k\": 3,\n",
        "\n",
        "    # Evaluation config\n",
        "    # \"eval_llm_type\": \"kscope\",\n",
        "    # \"eval_llm_name\": \"Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"eval_llm_type\": \"vertex\",\n",
        "    \"eval_llm_name\": \"gemini-2.0-flash-001\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also provided: Using `text-embedding-005` for embeddings, Gemini 2.0 Flash for generation,"
      ],
      "metadata": {
        "id": "cH-1XvGAiNIA"
      },
      "id": "cH-1XvGAiNIA"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_cfg = {\n",
        "    # Node parser config\n",
        "    \"chunk_size\": 256,\n",
        "    \"chunk_overlap\": 0,\n",
        "\n",
        "    # Embedding model config\n",
        "    # \"embed_model_type\": \"hf\",\n",
        "    # \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n",
        "    \"embed_model_type\": \"vertex\",\n",
        "    \"embed_model_name\": \"text-embedding-005\",\n",
        "\n",
        "    # LLM config\n",
        "    # \"llm_type\": \"kscope\",\n",
        "    # \"llm_name\": \"Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"llm_type\": \"vertex\",\n",
        "    \"llm_name\": \"gemini-2.0-flash-001\",\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"temperature\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 50,\n",
        "    \"do_sample\": False,\n",
        "\n",
        "    # Vector DB config\n",
        "    \"vector_db_type\": \"weaviate\", # \"weaviate\"\n",
        "    # \"vector_db_type\": \"vertex\",\n",
        "    \"vector_db_name\": \"Pubmed_QA\",\n",
        "    # MODIFY THIS\n",
        "    \"weaviate_url\": \"https://ds4tx7ttr3ciaui5obmowg.c0.us-east1.gcp.weaviate.cloud\",\n",
        "\n",
        "    # Retriever and query config\n",
        "    \"retriever_type\": \"vector_index\", # \"vector_index\"\n",
        "    \"retriever_similarity_top_k\": 5,\n",
        "    \"query_mode\": \"default\", # \"default\", \"hybrid\" - jkwng: changed to default\n",
        "    \"hybrid_search_alpha\": 0.0, # float from 0.0 (sparse search - bm25) to 1.0 (vector search)\n",
        "    \"response_mode\": \"compact\",\n",
        "    \"use_reranker\": False,\n",
        "    \"rerank_top_k\": 3,\n",
        "\n",
        "    # Evaluation config\n",
        "    # \"eval_llm_type\": \"kscope\",\n",
        "    # \"eval_llm_name\": \"Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"eval_llm_type\": \"vertex\",\n",
        "    \"eval_llm_name\": \"gemini-2.0-flash-001\"\n",
        "}"
      ],
      "metadata": {
        "id": "64LzeyZ7ipkD"
      },
      "id": "64LzeyZ7ipkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7789ee5b-ceef-48c0-9e09-11ea2051233b",
      "metadata": {
        "id": "7789ee5b-ceef-48c0-9e09-11ea2051233b"
      },
      "source": [
        "#### Read Weaviate Key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: load weaviate API key from secret manager*"
      ],
      "metadata": {
        "id": "Ejm-SqmSJZ8J"
      },
      "id": "Ejm-SqmSJZ8J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed2f405b-4c51-4174-ae8b-03029adec4a8",
      "metadata": {
        "id": "ed2f405b-4c51-4174-ae8b-03029adec4a8"
      },
      "outputs": [],
      "source": [
        "from google.cloud import secretmanager\n",
        "\n",
        "client = secretmanager.SecretManagerServiceClient()\n",
        "\n",
        "# Access the secret\n",
        "name = f\"projects/{PROJECT_ID}/secrets/weaviate_key/versions/latest\"\n",
        "response = client.access_secret_version(request={\"name\": name})\n",
        "\n",
        "# Extract and print the secret value\n",
        "weaviate_key = response.payload.data.decode(\"UTF-8\")\n",
        "WEAVIATE_API_KEY = weaviate_key\n",
        "\n",
        "# try:\n",
        "#     f = open(Path.home() / \".weaviate.key\", \"r\")\n",
        "#     f.close()\n",
        "# except Exception as err:\n",
        "#     print(f\"Could not read your Weaviate key. Please make sure this is available in plain text under your home directory in ~/.weaviate.key: {err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d63ff4f-fdcb-48db-8c88-87b602fb0f86",
      "metadata": {
        "id": "7d63ff4f-fdcb-48db-8c88-87b602fb0f86"
      },
      "source": [
        "#### Preliminary config checks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: validate_rag_cfg from utils.rag_utils.py*\n",
        "def validate_rag_cfg(cfg):\n",
        "    if cfg[\"query_mode\"] == \"hybrid\":\n",
        "        assert (\n",
        "            cfg[\"hybrid_search_alpha\"] is not None\n",
        "        ), \"hybrid_search_alpha cannot be None if query_mode is set to 'hybrid'\"\n",
        "    if cfg[\"vector_db_type\"] == \"weaviate\":\n",
        "        assert (\n",
        "            cfg[\"weaviate_url\"] is not None\n",
        "        ), \"weaviate_url cannot be None for weaviate vector db\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "DczvCm8KJezw"
      },
      "id": "DczvCm8KJezw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e83dc8-9449-41d6-899d-0189bacaf437",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08e83dc8-9449-41d6-899d-0189bacaf437",
        "outputId": "d18af375-e68f-4f11-bdc1-d9653d95916b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chunk_overlap': 0,\n",
            " 'chunk_size': 256,\n",
            " 'do_sample': False,\n",
            " 'embed_model_name': 'text-embedding-005',\n",
            " 'embed_model_type': 'vertex',\n",
            " 'eval_llm_name': 'gemini-2.0-flash-001',\n",
            " 'eval_llm_type': 'vertex',\n",
            " 'hybrid_search_alpha': 0.0,\n",
            " 'llm_name': 'gemini-2.0-flash-001',\n",
            " 'llm_type': 'vertex',\n",
            " 'max_new_tokens': 256,\n",
            " 'query_mode': 'default',\n",
            " 'rerank_top_k': 3,\n",
            " 'response_mode': 'compact',\n",
            " 'retriever_similarity_top_k': 5,\n",
            " 'retriever_type': 'vector_index',\n",
            " 'temperature': 0.0,\n",
            " 'top_k': 50,\n",
            " 'top_p': 1.0,\n",
            " 'use_reranker': False,\n",
            " 'vector_db_name': 'Pubmed_QA',\n",
            " 'vector_db_type': 'weaviate',\n",
            " 'weaviate_url': 'https://ds4tx7ttr3ciaui5obmowg.c0.us-east1.gcp.weaviate.cloud'}\n"
          ]
        }
      ],
      "source": [
        "validate_rag_cfg(rag_cfg)\n",
        "pprint(rag_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8672715b-12b7-424d-8410-859ff336a757",
      "metadata": {
        "id": "8672715b-12b7-424d-8410-859ff336a757"
      },
      "source": [
        "## STAGE 1 - Load dataset and documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe5fd2a-f974-4bb5-a678-7110aba1bc32",
      "metadata": {
        "id": "8fe5fd2a-f974-4bb5-a678-7110aba1bc32"
      },
      "source": [
        "#### 1. Load PubMed QA dataset\n",
        "PubMedQA ([github](https://github.com/pubmedqa/pubmedqa)) is a biomedical question answering dataset. Each instance consists of a question, a context (extracted from PubMed abstracts), a long answer and a yes/no/maybe answer. We make use of the test split of [this](https://huggingface.co/datasets/bigbio/pubmed_qa) huggingface dataset for this notebook.\n",
        "\n",
        "**The context for each instance is stored as a text file** (referred to as documents), to align the task as a standard RAG use-case."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: task_dataset.py*\n",
        "import os\n",
        "import json\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "class PubMedQATaskDataset(data.Dataset):\n",
        "    def __init__(self, name, all_folds=False, split=\"test\"):\n",
        "        self.name = name\n",
        "        subset_str = \"pubmed_qa_labeled_fold{fold_id}\"\n",
        "        folds = [0] if not all_folds else list(range(10))\n",
        "\n",
        "        bigbio_data = []\n",
        "        source_data = []\n",
        "        for fold_id in folds:\n",
        "            bb_data = load_dataset(\n",
        "                self.name,\n",
        "                f\"{subset_str.format(fold_id=fold_id)}_bigbio_qa\",\n",
        "                split=split,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            s_data = load_dataset(\n",
        "                self.name,\n",
        "                f\"{subset_str.format(fold_id=fold_id)}_source\",\n",
        "                split=split,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            bigbio_data.append(bb_data)\n",
        "            source_data.append(s_data)\n",
        "        bigbio_data = concatenate_datasets(bigbio_data)\n",
        "        source_data = concatenate_datasets(source_data)\n",
        "\n",
        "        keys_to_keep = [\"id\", \"question\", \"context\", \"answer\", \"LONG_ANSWER\"]\n",
        "        data_elms = []\n",
        "        for elm_idx in tqdm(range(len(bigbio_data)), desc=\"Preparing data\"):\n",
        "            data_elms.append({k: bigbio_data[elm_idx][k] for k in keys_to_keep[:4]})\n",
        "            data_elms[-1].update(\n",
        "                {keys_to_keep[-1].lower(): source_data[elm_idx][keys_to_keep[-1]]}\n",
        "            )\n",
        "\n",
        "        self.data = data_elms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def mock_knowledge_base(\n",
        "        self,\n",
        "        output_dir,\n",
        "        one_file_per_sample=False,\n",
        "        samples_per_file=500,\n",
        "        sep=\"\\n\",\n",
        "        jsonl=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Write PubMed contexts to a text file, newline seperated\n",
        "        \"\"\"\n",
        "        pubmed_kb_dir = os.path.join(output_dir, \"pubmed_doc\")\n",
        "        os.makedirs(pubmed_kb_dir, exist_ok=True)\n",
        "\n",
        "        file_ext = \"jsonl\" if jsonl else \"txt\"\n",
        "\n",
        "        if not one_file_per_sample:\n",
        "            context_str = \"\"\n",
        "            context_files = []\n",
        "            for idx in range(len(self.data)):\n",
        "                if (idx + 1) % samples_per_file == 0:\n",
        "                    context_files.append(context_str.rstrip(sep))\n",
        "                else:\n",
        "                    if jsonl:\n",
        "                        context_elm_str = json.dumps(\n",
        "                            {\n",
        "                                \"id\": self.data[idx][\"id\"],\n",
        "                                \"context\": self.data[idx][\"context\"],\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        context_elm_str = self.data[idx][\"context\"]\n",
        "                    context_str += f\"{context_elm_str}{sep}\"\n",
        "\n",
        "            for file_idx in range(len(context_files)):\n",
        "                filepath = os.path.join(pubmed_kb_dir, f\"context{file_idx}.{file_ext}\")\n",
        "                with open(filepath, \"w\") as f:\n",
        "                    f.write(context_files[file_idx])\n",
        "\n",
        "        else:\n",
        "            assert not jsonl, \"Does not support jsonl if one_file_per_sample is True\"\n",
        "            for idx in range(len(self.data)):\n",
        "                filepath = os.path.join(\n",
        "                    pubmed_kb_dir, f'{self.data[idx][\"id\"]}.{file_ext}'\n",
        "                )\n",
        "                with open(filepath, \"w\") as f:\n",
        "                    f.write(self.data[idx][\"context\"])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MONSnHCe7DsJ"
      },
      "id": "MONSnHCe7DsJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd814ec-3084-4a5f-9fa9-8def165fe77d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abd814ec-3084-4a5f-9fa9-8def165fe77d",
        "outputId": "a20973bd-d27b-40b9-9ef8-fc3df2488296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading PubMed QA data ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing data: 100%|██████████| 500/500 [00:00<00:00, 1133.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data size: 500\n"
          ]
        }
      ],
      "source": [
        "print('Loading PubMed QA data ...')\n",
        "pubmed_data = PubMedQATaskDataset('bigbio/pubmed_qa')\n",
        "print(f\"Loaded data size: {len(pubmed_data)}\")\n",
        "pubmed_data.mock_knowledge_base(output_dir='./data', one_file_per_sample=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: TODO: write knowledge base to GCS - to simulate loading knowledge base from object storage*"
      ],
      "metadata": {
        "id": "8W0xH_NVAHta"
      },
      "id": "8W0xH_NVAHta"
    },
    {
      "cell_type": "markdown",
      "id": "0139f09c-bf13-4d4a-9637-c7be7e165ad8",
      "metadata": {
        "id": "0139f09c-bf13-4d4a-9637-c7be7e165ad8"
      },
      "source": [
        "#### 2. Load documents\n",
        "All metadata is excluded by default. Set the *exclude_llm_metadata_keys* and *exclude_embed_metadata_keys* flags to *false* for including it. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents.html) and the *DocumentReader* class from *rag_utils.py* for further details."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: DocumentReader from utils.rag_utils.py*\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader\n",
        ")\n",
        "\n",
        "from llama_index.readers.json import JSONReader\n",
        "\n",
        "class DocumentReader:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dir,\n",
        "        exclude_llm_metadata_keys=True,\n",
        "        exclude_embed_metadata_keys=True,\n",
        "    ):\n",
        "        self.input_dir = input_dir\n",
        "        self._file_ext = os.path.splitext(os.listdir(input_dir)[0])[1]\n",
        "\n",
        "        self.exclude_llm_metadata_keys = exclude_llm_metadata_keys\n",
        "        self.exclude_embed_metadata_keys = exclude_embed_metadata_keys\n",
        "\n",
        "    def load_data(self):\n",
        "        docs = None\n",
        "        # Use reader based on file extension of documents\n",
        "        # Only support '.txt' files as of now\n",
        "        if self._file_ext == \".txt\":\n",
        "            reader = SimpleDirectoryReader(input_dir=self.input_dir)\n",
        "            docs = reader.load_data()\n",
        "        elif self._file_ext == \".jsonl\":\n",
        "            reader = JSONReader()\n",
        "            docs = []\n",
        "            for file in os.listdir(self.input_dir):\n",
        "                docs.extend(\n",
        "                    reader.load_data(os.path.join(self.input_dir, file), is_jsonl=True)\n",
        "                )\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Does not support {self._file_ext} file extension for document files.\"\n",
        "            )\n",
        "\n",
        "        # Can choose if metadata need to be included as input when passing the doc to LLM or embeddings:\n",
        "        # https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents.html\n",
        "        # Exclude metadata keys from embeddings or LLMs based on flag\n",
        "        if docs is not None:\n",
        "            all_metadata_keys = list(docs[0].metadata.keys())\n",
        "            if self.exclude_llm_metadata_keys:\n",
        "                for doc in docs:\n",
        "                    doc.excluded_llm_metadata_keys = all_metadata_keys\n",
        "            if self.exclude_embed_metadata_keys:\n",
        "                for doc in docs:\n",
        "                    doc.excluded_embed_metadata_keys = all_metadata_keys\n",
        "\n",
        "        return docs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xd8KZP8g8w6U"
      },
      "id": "xd8KZP8g8w6U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b8ff42-45ee-4ad2-bf72-ccca100e1286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2b8ff42-45ee-4ad2-bf72-ccca100e1286",
        "outputId": "ec1d3af7-908d-49fe-949a-e30776059126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents ...\n",
            "No. of documents loaded: 500\n"
          ]
        }
      ],
      "source": [
        "print('Loading documents ...')\n",
        "reader = DocumentReader(input_dir=\"./data/pubmed_doc\")\n",
        "docs = reader.load_data()\n",
        "print(f'No. of documents loaded: {len(docs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: TODO: load the knowledge base from GCS*"
      ],
      "metadata": {
        "id": "WFdv0ymAASmk"
      },
      "id": "WFdv0ymAASmk"
    },
    {
      "cell_type": "markdown",
      "id": "c6db76ff-1c8c-4ada-8c3d-30b2ef7bca30",
      "metadata": {
        "id": "c6db76ff-1c8c-4ada-8c3d-30b2ef7bca30"
      },
      "source": [
        "## STAGE 2 - Load node parser, embedding, LLM and set service context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e540f6-2522-4d69-89e6-3233f665c589",
      "metadata": {
        "id": "77e540f6-2522-4d69-89e6-3233f665c589"
      },
      "source": [
        "#### 1. Load node parser to split documents into smaller chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd11331-fe79-4100-bcd9-127838159e1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acd11331-fe79-4100-bcd9-127838159e1c",
        "outputId": "8d17577b-a6f1-4f08-ff86-a62c48aa5230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading node parser ...\n"
          ]
        }
      ],
      "source": [
        "print('Loading node parser ...')\n",
        "node_parser = SentenceSplitter(chunk_size=rag_cfg['chunk_size'], chunk_overlap=rag_cfg['chunk_overlap'])\n",
        "nodes = node_parser.get_nodes_from_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018b8905-8a8e-4495-90f4-4e69811d9d19",
      "metadata": {
        "id": "018b8905-8a8e-4495-90f4-4e69811d9d19"
      },
      "source": [
        "#### 2. Load embedding model\n",
        "LlamaIndex supports embedding models from OpenAI, Cohere, HuggingFace, etc. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#custom-embedding-model) for building a custom embedding model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: RAGEmbedding from utils.rag_utils.py - update to support using Vertex AI Gemini Embeddings models*\n",
        "from llama_index.embeddings.vertex_endpoint import VertexEndpointEmbedding\n",
        "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.embeddings.text_embeddings_inference import TextEmbeddingsInference\n",
        "import google.auth\n",
        "\n",
        "credentials, project_id = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "credentials.refresh(auth_req)\n",
        "\n",
        "class RAGEmbedding:\n",
        "    \"\"\"\n",
        "    LlamaIndex supports embedding models from OpenAI, Cohere, HuggingFace, etc.\n",
        "    https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\n",
        "    We can also build out custom embedding model:\n",
        "    https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#custom-embedding-model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_type, model_name):\n",
        "        self.model_type = model_type\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def load_model(self, **kwargs):\n",
        "        print(f\"Loading {self.model_type} embedding model ...\")\n",
        "        if self.model_type == \"hf\":\n",
        "            # Using bge base HuggingFace embeddings, can choose others based on leaderboard:\n",
        "            # https://huggingface.co/spaces/mteb/leaderboard\n",
        "            model = HuggingFaceEmbedding(\n",
        "                model_name=self.model_name,\n",
        "                device=\"cuda\",\n",
        "                trust_remote_code=True,\n",
        "            )  # max_length does not have any effect?\n",
        "        elif self.model_type == \"vertex\":\n",
        "            model = GoogleGenAIEmbedding(\n",
        "                model_name=self.model_name,\n",
        "                vertexai_config={\n",
        "                  \"project\": PROJECT_ID,\n",
        "                  \"location\": REGION,\n",
        "                },\n",
        "                embed_batch_size=100,\n",
        "            )\n",
        "        elif self.model_type == \"vertex-endpoint\":\n",
        "            model = VertexEndpointEmbedding(\n",
        "                endpoint_id=kwargs[\"embed_model_endpoint_id\"],\n",
        "                project_id=PROJECT_ID,\n",
        "                location=REGION,\n",
        "                endpoint_kwargs={\n",
        "                    \"use_dedicated_endpoint\": kwargs[\"embed_model_use_dedicated_endpoint\"],\n",
        "                },\n",
        "            )  # max_length does not have any effect?\n",
        "        elif self.model_type == \"openai\":\n",
        "            # TODO - Add OpenAI embedding model\n",
        "            # embed_model = OpenAIEmbedding()\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return embed_model"
      ],
      "metadata": {
        "id": "uQeTSmv6Azxa"
      },
      "id": "uQeTSmv6Azxa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cae31175-725c-46f9-ae43-5dc80f604649",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cae31175-725c-46f9-ae43-5dc80f604649",
        "outputId": "148828de-fad0-454f-9b09-2a8ac135e000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vertex embedding model ...\n"
          ]
        }
      ],
      "source": [
        "embed_model = RAGEmbedding(model_type=rag_cfg['embed_model_type'], model_name=rag_cfg['embed_model_name']).load_model(**rag_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6446adf-983e-451c-b5dd-9a178e3b1af7",
      "metadata": {
        "id": "b6446adf-983e-451c-b5dd-9a178e3b1af7"
      },
      "source": [
        "#### 3. Load LLM for generation\n",
        "LlamaIndex supports LLMs from OpenAI, Cohere, HuggingFace, AI21, etc. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html#example-using-a-custom-llm-model-advanced) for loading a custom LLM model for generation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: RAGLLM from utils.hosting_utils.py - updated for Gemini on Vertex*\n",
        "\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.llms.vertex import Vertex\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "class RAGLLM:\n",
        "    \"\"\"\n",
        "    LlamaIndex supports OpenAI, Cohere, AI21 and HuggingFace LLMs\n",
        "    https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_type, llm_name, api_base=None, api_key=None):\n",
        "        self.llm_type = llm_type\n",
        "        self.llm_name = llm_name\n",
        "\n",
        "        self._api_base = api_base\n",
        "        self._api_key = api_key\n",
        "\n",
        "        self.local_model_path = \"/model-weights\"\n",
        "\n",
        "    def load_model(self, **kwargs):\n",
        "        print(f\"Configuring {self.llm_type} LLM model ...\")\n",
        "        gen_arg_keys = [\"temperature\", \"top_p\", \"top_k\", \"do_sample\"]\n",
        "        gen_kwargs = {k: v for k, v in kwargs.items() if k in gen_arg_keys}\n",
        "        if self.llm_type == \"local\":\n",
        "            # Using local HuggingFace LLM stored at /model-weights\n",
        "            llm = HuggingFaceLLM(\n",
        "                tokenizer_name=f\"{self.local_model_path}/{self.llm_name}\",\n",
        "                model_name=f\"{self.local_model_path}/{self.llm_name}\",\n",
        "                device_map=\"auto\",\n",
        "                context_window=4096,\n",
        "                max_new_tokens=kwargs[\"max_new_tokens\"],\n",
        "                generate_kwargs=gen_kwargs,\n",
        "                # model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
        "            )\n",
        "        # jkwng: add vertex support\n",
        "        elif self.llm_type in [\"vertex\"]:\n",
        "            llm = Vertex(\n",
        "                model=self.llm_name,\n",
        "                temperature=kwargs[\"temperature\"],\n",
        "                max_tokens=kwargs[\"max_new_tokens\"],\n",
        "            )\n",
        "        elif self.llm_type in [\"vertex-endpoint\"]:\n",
        "            ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "                PROJECT_ID, REGION, kwargs[\"llm_endpoint_id\"] # llm_name is the endpoint id\n",
        "            )\n",
        "            BASE_URL = (\n",
        "              f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "            )\n",
        "            try:\n",
        "                if kwargs[\"llm_use_dedicated_endpoint\"]:\n",
        "                    BASE_URL = f\"https://{kwargs['llm_dedicated_dns']}/v1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "            except NameError:\n",
        "                pass\n",
        "            llm = OpenAILike(\n",
        "                model=self.llm_name,\n",
        "                temperature=kwargs[\"temperature\"],\n",
        "                max_tokens=kwargs[\"max_new_tokens\"],\n",
        "                api_base=BASE_URL,\n",
        "                api_key=creds.token,\n",
        "                is_chat_model=True,\n",
        "                top_p=kwargs[\"top_p\"],\n",
        "                top_k=kwargs[\"top_k\"],\n",
        "            )\n",
        "        elif self.llm_type in [\"openai\", \"kscope\"]:\n",
        "            llm = OpenAILike(\n",
        "                model=self.llm_name,\n",
        "                api_base=self._api_base,\n",
        "                api_key=self._api_key,\n",
        "                is_chat_model=True,\n",
        "                temperature=kwargs[\"temperature\"],\n",
        "                max_tokens=kwargs[\"max_new_tokens\"],\n",
        "                top_p=kwargs[\"top_p\"],\n",
        "                top_k=kwargs[\"top_k\"],\n",
        "            )\n",
        "        return llm"
      ],
      "metadata": {
        "id": "pwqNy5v9DH9A",
        "cellView": "form"
      },
      "id": "pwqNy5v9DH9A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99126d7-48d6-4551-9c64-2044f7962ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d99126d7-48d6-4551-9c64-2044f7962ffd",
        "outputId": "e2fcf332-f6f8-48cf-d27a-e5d0dd38488d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring vertex LLM model ...\n"
          ]
        }
      ],
      "source": [
        "llm = RAGLLM(\n",
        "    llm_type=rag_cfg['llm_type'],\n",
        "    llm_name=rag_cfg['llm_name'],\n",
        "    # api_base=GENERATOR_BASE_URL,\n",
        "    # api_key=OPENAI_API_KEY,\n",
        ").load_model(**rag_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb554bd-9df1-444c-b75a-373e117eee06",
      "metadata": {
        "id": "feb554bd-9df1-444c-b75a-373e117eee06"
      },
      "source": [
        "#### 4. Use ```Settings``` to set the node parser, embedding model, LLM, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c62a809-0558-4e29-ae0d-451259663f60",
      "metadata": {
        "id": "4c62a809-0558-4e29-ae0d-451259663f60"
      },
      "outputs": [],
      "source": [
        "Settings.text_splitter = node_parser\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dceb1e4-5ed6-47cb-987f-93f186387269",
      "metadata": {
        "id": "5dceb1e4-5ed6-47cb-987f-93f186387269"
      },
      "source": [
        "## STAGE 3 - Create index using the appropriate vector store\n",
        "All vector stores supported by LlamaIndex along with their available features are listed [here](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html).\n",
        "\n",
        "If you are using LangChain, the supported vector stores can be found [here](https://python.langchain.com/docs/modules/data_connection/vectorstores/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: Llama Index + Vertex AI Vector Store integration seems to be broken and has the following things that do not work:*\n",
        "\n",
        "- *Batch Updates to a staging bucket gives an error, only Streaming Index works*\n",
        "- *Retrieval is broken - the API has changed but the library has not been updated*\n",
        "\n",
        "*For the purposes of the notebook - we will use Weaviate*"
      ],
      "metadata": {
        "id": "rgiGDroMCANj"
      },
      "id": "rgiGDroMCANj"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: RAGIndex from utils.storage_utils.py - modified to use Vertex Vector Store*\n",
        "import faiss\n",
        "import os\n",
        "import weaviate\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
        "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore\n",
        "\n",
        "# from .rag_utils import get_embed_model_dim\n",
        "def get_embed_model_dim(embed_model):\n",
        "    embed_out = embed_model.get_text_embedding(\"Dummy Text\")\n",
        "    return len(embed_out)\n",
        "\n",
        "class RAGIndex:\n",
        "    \"\"\"\n",
        "    Use storage context to set custom vector store\n",
        "    Available options: https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html\n",
        "    Use Chroma: https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo.html\n",
        "    LangChain vector stores: https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_type, db_name):\n",
        "        self.db_type = db_type\n",
        "        self.db_name = db_name\n",
        "        self._persist_dir = f\"./.{db_type}_index_store/\"\n",
        "\n",
        "    def create_index(self, docs, save=True, **kwargs):\n",
        "        # Only supports Weaviate as of now\n",
        "        if self.db_type == \"weaviate\":\n",
        "            # with open(Path.home() / \".weaviate.key\", \"r\") as f:\n",
        "            #     weaviate_api_key = f.read().rstrip(\"\\n\")\n",
        "            weaviate_client = weaviate.connect_to_wcs(\n",
        "                cluster_url=kwargs[\"weaviate_url\"],\n",
        "                auth_credentials=weaviate.auth.AuthApiKey(WEAVIATE_API_KEY),\n",
        "            )\n",
        "            vector_store = WeaviateVectorStore(\n",
        "                weaviate_client=weaviate_client,\n",
        "                index_name=self.db_name,\n",
        "            )\n",
        "        elif self.db_type == \"local\":\n",
        "            # Use FAISS vector database for local index\n",
        "            faiss_dim = get_embed_model_dim(kwargs[\"embed_model\"])\n",
        "            faiss_index = faiss.IndexFlatL2(faiss_dim)\n",
        "            vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "        # jkwng: added Vertex AI Vector Search support here\n",
        "        elif self.db_type == \"vertex\":\n",
        "          # check if storage bucket exists\n",
        "          bucket_names = [\n",
        "              bucket.name for bucket in storage.Client().list_buckets()\n",
        "          ]\n",
        "\n",
        "          dst_bucket = f\"jkwng-{self.db_name.replace('_', '-').lower()}\"\n",
        "\n",
        "          if dst_bucket not in bucket_names:\n",
        "              print(f\"Creating bucket {dst_bucket} ...\")\n",
        "              storage.Client().create_bucket(dst_bucket, location=REGION)\n",
        "              print(f\"Bucket {dst_bucket} created\")\n",
        "          else:\n",
        "              print(f\"Bucket {dst_bucket} exists\")\n",
        "\n",
        "          # check if index exists already in vertex\n",
        "          index_names = [\n",
        "              index.resource_name\n",
        "              for index in aiplatform.MatchingEngineIndex.list(\n",
        "                  filter=f\"display_name={self.db_name}\"\n",
        "              )\n",
        "          ]\n",
        "\n",
        "          # create the index if it doesn't exist\n",
        "          if len(index_names) == 0:\n",
        "              print(f\"Creating Vector Search index {self.db_name} ...\")\n",
        "              vs_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "                  display_name=self.db_name,\n",
        "                  dimensions=768,\n",
        "                  approximate_neighbors_count=100,\n",
        "                  distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
        "                  shard_size=\"SHARD_SIZE_SMALL\",\n",
        "                  index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} created with resource name {vs_index.resource_name}\"\n",
        "              )\n",
        "          else:\n",
        "              vs_index = aiplatform.MatchingEngineIndex(index_name=index_names[0])\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} exists with resource name {vs_index.resource_name}\"\n",
        "              )\n",
        "\n",
        "          # create an endpoint to serve the index\n",
        "          endpoint_names = [\n",
        "              endpoint.resource_name\n",
        "              for endpoint in aiplatform.MatchingEngineIndexEndpoint.list(\n",
        "                  filter=f\"display_name={self.db_name}\"\n",
        "              )\n",
        "          ]\n",
        "\n",
        "          if len(endpoint_names) == 0:\n",
        "              print(\n",
        "                  f\"Creating Vector Search index endpoint {self.db_name} ...\"\n",
        "              )\n",
        "              vs_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "                  display_name=self.db_name, public_endpoint_enabled=True\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index endpoint {vs_endpoint.display_name} created with resource name {vs_endpoint.resource_name}\"\n",
        "              )\n",
        "          else:\n",
        "              vs_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
        "                  index_endpoint_name=endpoint_names[0]\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index endpoint {vs_endpoint.display_name} exists with resource name {vs_endpoint.resource_name}\"\n",
        "              )\n",
        "\n",
        "          # check if endpoint exists\n",
        "          index_endpoints = [\n",
        "              (deployed_index.index_endpoint, deployed_index.deployed_index_id)\n",
        "              for deployed_index in vs_index.deployed_indexes\n",
        "          ]\n",
        "\n",
        "          if len(index_endpoints) == 0:\n",
        "              print(\n",
        "                  f\"Deploying Vector Search index {vs_index.display_name} at endpoint {vs_endpoint.display_name} ...\"\n",
        "              )\n",
        "              vs_deployed_index = vs_endpoint.deploy_index(\n",
        "                  index=vs_index,\n",
        "                  deployed_index_id=self.db_name,\n",
        "                  display_name=self.db_name,\n",
        "                  machine_type=\"e2-standard-2\",\n",
        "                  min_replica_count=1,\n",
        "                  max_replica_count=1,\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} is deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "              )\n",
        "          else:\n",
        "              vs_deployed_index = aiplatform.MatchingEngineIndexEndpoint(\n",
        "                  index_endpoint_name=index_endpoints[0][0]\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} is already deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "              )\n",
        "\n",
        "          # setup storage\n",
        "          vector_store = VertexAIVectorStore(\n",
        "              project_id=PROJECT_ID,\n",
        "              region=REGION,\n",
        "              index_id=vs_index.resource_name,\n",
        "              endpoint_id=vs_endpoint.resource_name,\n",
        "              gcs_bucket_name=dst_bucket,\n",
        "          )\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Incorrect vector db type - {self.db_type}\")\n",
        "\n",
        "        if os.path.isdir(self._persist_dir):\n",
        "            # Load if index already saved\n",
        "            print(f\"Loading index from {self._persist_dir} ...\")\n",
        "            storage_context = StorageContext.from_defaults(\n",
        "                vector_store=vector_store,\n",
        "                persist_dir=self._persist_dir,\n",
        "            )\n",
        "            index = load_index_from_storage(storage_context)\n",
        "        else:\n",
        "            # Re-index\n",
        "            print(\"Creating new index ...\")\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                docs, storage_context=storage_context\n",
        "            )\n",
        "            if save:\n",
        "                os.makedirs(self._persist_dir, exist_ok=True)\n",
        "                index.storage_context.persist(persist_dir=self._persist_dir)\n",
        "\n",
        "        return index"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uNBIQ24qWYt9"
      },
      "id": "uNBIQ24qWYt9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23824c74-c247-46ff-86fc-dcf1a8bd6fcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23824c74-c247-46ff-86fc-dcf1a8bd6fcd",
        "outputId": "9006c6e9-3ddc-4fb2-b51a-b491280605a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading index from ./.weaviate_index_store/ ...\n"
          ]
        }
      ],
      "source": [
        "index = RAGIndex(\n",
        "    db_type=rag_cfg['vector_db_type'],\n",
        "    db_name=rag_cfg['vector_db_name'],\n",
        ").create_index(docs, weaviate_url=rag_cfg[\"weaviate_url\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b175a4a5-0a66-41b3-a848-850ce048bc6c",
      "metadata": {
        "id": "b175a4a5-0a66-41b3-a848-850ce048bc6c"
      },
      "source": [
        "## STAGE 4 - Build query engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608d105b-87ca-4bac-a1b0-254f73297b0d",
      "metadata": {
        "id": "608d105b-87ca-4bac-a1b0-254f73297b0d"
      },
      "source": [
        "Now build a query engine using *retriever* and *response_synthesizer*. LlamaIndex also supports different types of [retrievers](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html) and [response modes](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#configuring-the-response-mode) for various use-cases.\n",
        "\n",
        "[Weaviate hybrid search](https://weaviate.io/blog/hybrid-search-explained) explains how dense and sparse search is combined."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng - RAGQueryEngine from utils.rag_utils.py - add support for Vertex AI*\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "from llama_index.core import (\n",
        "    PromptTemplate,\n",
        "    get_response_synthesizer,\n",
        ")\n",
        "\n",
        "class RAGQueryEngine:\n",
        "    \"\"\"\n",
        "    https://docs.llamaindex.ai/en/stable/understanding/querying/querying.html\n",
        "    TODO - Check other args for RetrieverQueryEngine\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, retriever_type, vector_index):\n",
        "        self.retriever_type = retriever_type\n",
        "        self.index = vector_index\n",
        "        self.retriever = None\n",
        "        self.node_postprocessor = None\n",
        "        self.response_synthesizer = None\n",
        "\n",
        "    def create(self, similarity_top_k, response_mode, **kwargs):\n",
        "        self.set_retriever(similarity_top_k, **kwargs)\n",
        "        self.set_response_synthesizer(response_mode=response_mode)\n",
        "        if kwargs[\"use_reranker\"]:\n",
        "            self.set_node_postprocessors(rerank_top_k=kwargs[\"rerank_top_k\"])\n",
        "        query_engine = RetrieverQueryEngine(\n",
        "            retriever=self.retriever,\n",
        "            node_postprocessors=self.node_postprocessor,\n",
        "            response_synthesizer=self.response_synthesizer,\n",
        "        )\n",
        "        return query_engine\n",
        "\n",
        "    def set_retriever(self, similarity_top_k, **kwargs):\n",
        "        # Other retrievers can be used based on the type of index: List, Tree, Knowledge Graph, etc.\n",
        "        # https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html\n",
        "        # Find LlamaIndex equivalents for the following:\n",
        "        # Check MultiQueryRetriever from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
        "        # Check Contextual compression from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/\n",
        "        # Check Ensemble Retriever from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble\n",
        "        # Check self-query from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/self_query\n",
        "        # Check WebSearchRetriever from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/web_research\n",
        "        if self.retriever_type == \"vector_index\":\n",
        "            self.retriever = VectorIndexRetriever(\n",
        "                index=self.index,\n",
        "                similarity_top_k=similarity_top_k,\n",
        "                vector_store_query_mode=kwargs[\"query_mode\"],\n",
        "                alpha=kwargs[\"hybrid_search_alpha\"],\n",
        "            )\n",
        "        elif self.retriever_type == \"bm25\":\n",
        "            self.retriever = BM25Retriever(\n",
        "                nodes=kwargs[\"nodes\"],\n",
        "                tokenizer=kwargs[\"tokenizer\"],\n",
        "                similarity_top_k=similarity_top_k,\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Incorrect retriever type - {self.retriever_type}\"\n",
        "            )\n",
        "\n",
        "    def set_node_postprocessors(self, rerank_top_k=2):\n",
        "        # Node postprocessor: Porcessing nodes after retrieval before passing to the LLM for generation\n",
        "        # Re-ranking step can be performed here!\n",
        "        # Nodes can be re-ordered to include more relevant ones at the top: https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder\n",
        "        # https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors.html\n",
        "\n",
        "        self.node_postprocessor = [LLMRerank(top_n=rerank_top_k)]\n",
        "\n",
        "    def set_response_synthesizer(self, response_mode):\n",
        "        # Other response modes: https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#configuring-the-response-mode\n",
        "        qa_prompt_tmpl = (\n",
        "            \"Context information is below.\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"Given the context information and not prior knowledge, answer the query while providing an explanation. \"\n",
        "            \"If your answer is in favour of the query, end your response with 'yes' otherwise end your response with 'no'.\\n\"\n",
        "            \"Query: {query_str}\\n\"\n",
        "            \"Answer: \"\n",
        "        )\n",
        "        qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl)\n",
        "\n",
        "        self.response_synthesizer = get_response_synthesizer(\n",
        "            text_qa_template=qa_prompt_tmpl,\n",
        "            response_mode=response_mode,\n",
        "        )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tsIR8wKb0-E0"
      },
      "id": "tsIR8wKb0-E0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce527d9f-9f18-444a-ab77-b110a5277a1c",
      "metadata": {
        "id": "ce527d9f-9f18-444a-ab77-b110a5277a1c"
      },
      "outputs": [],
      "source": [
        "def set_query_engine_args(rag_cfg, docs):\n",
        "    query_engine_args = {\n",
        "        \"similarity_top_k\": rag_cfg['retriever_similarity_top_k'],\n",
        "        \"response_mode\": rag_cfg['response_mode'],\n",
        "        \"use_reranker\": False,\n",
        "    }\n",
        "\n",
        "    # jkwng: add that retriever type vector_index could be \"vertex\" too\n",
        "    # jkwng: note we don't actually use hybrid search for vertex ai vector search\n",
        "    if (rag_cfg[\"retriever_type\"] == \"vector_index\") and (rag_cfg[\"vector_db_type\"] == \"weaviate\"):\n",
        "        query_engine_args.update({\n",
        "            \"query_mode\": rag_cfg[\"query_mode\"],\n",
        "            \"hybrid_search_alpha\": rag_cfg[\"hybrid_search_alpha\"]\n",
        "        })\n",
        "    elif (rag_cfg[\"retriever_type\"] == \"vector_index\") and (rag_cfg[\"vector_db_type\"] == \"vertex\"):\n",
        "        query_engine_args.update({\n",
        "            # jkwng: only default mode works with VVS\n",
        "            \"query_mode\": \"default\"\n",
        "        })\n",
        "    elif rag_cfg[\"retriever_type\"] == \"bm25\":\n",
        "        nodes = Settings.text_splitter.get_nodes_from_documents(docs)\n",
        "        tokenizer = Settings.embed_model._tokenizer\n",
        "        query_engine_args.update({\"nodes\": nodes, \"tokenizer\": tokenizer})\n",
        "\n",
        "    if rag_cfg[\"use_reranker\"]:\n",
        "        query_engine_args.update({\"use_reranker\": True, \"rerank_top_k\": rag_cfg[\"rerank_top_k\"]})\n",
        "\n",
        "    return query_engine_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47bc0bd-3d83-4dce-b488-38806eed654f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f47bc0bd-3d83-4dce-b488-38806eed654f",
        "outputId": "94e7b0e3-d416-4cca-ff8d-9d2c58958ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 0.0,\n",
            " 'query_mode': 'default',\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': False}\n"
          ]
        }
      ],
      "source": [
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5502edfd-75d0-4b48-b75d-46e6adf9447d",
      "metadata": {
        "id": "5502edfd-75d0-4b48-b75d-46e6adf9447d"
      },
      "outputs": [],
      "source": [
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index,\n",
        ").create(**query_engine_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e6df972-2515-4968-9ea9-e2604f6ab82a",
      "metadata": {
        "id": "9e6df972-2515-4968-9ea9-e2604f6ab82a"
      },
      "source": [
        "## STAGE 5 - Finally query the model !\n",
        "**Note:** We are using keyword based search or sparse search since *hybrid_search_alpha* is set to 0.0 by default."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffaa9ea9-a3c9-4205-b050-d347a9f425dd",
      "metadata": {
        "id": "ffaa9ea9-a3c9-4205-b050-d347a9f425dd"
      },
      "source": [
        "#### [TODO] Change seed to experiment with a different sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725791c9-371b-43d5-9d5f-9de5d01e04f0",
      "metadata": {
        "id": "725791c9-371b-43d5-9d5f-9de5d01e04f0"
      },
      "outputs": [],
      "source": [
        "random.seed(237)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7abee9-59f7-481b-a644-45c107d10686",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd7abee9-59f7-481b-a644-45c107d10686",
        "outputId": "b89d88b2-7395-4f6b-e47d-edefb2a4680d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': ['no'],\n",
            " 'context': 'Human immunodeficiency virus (HIV)-infected patients have '\n",
            "            'generally been excluded from transplantation. Recent advances in '\n",
            "            'the management and prognosis of these patients suggest that this '\n",
            "            'policy should be reevaluated. To explore the current views of '\n",
            "            'U.S. transplant centers toward transplanting asymptomatic '\n",
            "            'HIV-infected patients with end-stage renal disease, a written '\n",
            "            'survey was mailed to the directors of transplantation at all 248 '\n",
            "            'renal transplant centers in the United States. All 148 responding '\n",
            "            'centers said they require HIV testing of prospective kidney '\n",
            "            'recipients, and 84% of these centers would not transplant an '\n",
            "            'individual who refuses HIV testing. The vast majority of '\n",
            "            'responding centers would not transplant a kidney from a cadaveric '\n",
            "            '(88%) or a living donor (91%) into an asymptomatic HIV-infected '\n",
            "            'patient who is otherwise a good candidate for transplantation. '\n",
            "            'Among the few centers that would consider transplanting an '\n",
            "            'HIV-infected patient, not a single center had performed such a '\n",
            "            'transplant in the year prior to the survey. Most centers fear '\n",
            "            'that transplantation in the face of HIV infection would be '\n",
            "            'harmful to the individual, and some believe that it would be a '\n",
            "            'waste of precious organs.',\n",
            " 'id': '9603166',\n",
            " 'long_answer': 'The great majority of U.S. renal transplant centers will not '\n",
            "                'transplant kidneys to HIV-infected patients with end-stage '\n",
            "                'renal disease, even if their infection is asymptomatic. '\n",
            "                'However, advances in the management of HIV infection and a '\n",
            "                'review of relevant ethical issues suggest that this approach '\n",
            "                'should be reconsidered.',\n",
            " 'question': 'Should all human immunodeficiency virus-infected patients with '\n",
            "             'end-stage renal disease be excluded from transplantation?'}\n"
          ]
        }
      ],
      "source": [
        "sample_idx = random.randint(0, len(pubmed_data)-1)\n",
        "sample_elm = pubmed_data[sample_idx]\n",
        "pprint(sample_elm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: extract_yes_no from utils.rag_utils.py*\n",
        "import re\n",
        "\n",
        "def extract_yes_no(resp):\n",
        "    match_pat = r\"\\b(?:yes|no)\\b\"\n",
        "    match_txt = re.search(match_pat, resp, re.IGNORECASE)\n",
        "    if match_txt:\n",
        "        return match_txt.group(0)\n",
        "    return \"none\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "X7lMVucG3O4N"
      },
      "id": "X7lMVucG3O4N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8279db69-6525-401a-b39c-69f6b83cb0af",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8279db69-6525-401a-b39c-69f6b83cb0af",
        "outputId": "c9be420e-776b-4f45-c720-0c35cc11f9c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
            "\n",
            "RESPONSE:\n",
            "-------------------------\n",
            "Based on the provided context, the majority of U.S. transplant centers surveyed would not transplant a kidney into an asymptomatic HIV-infected patient. The primary reasons cited are concerns about harm to the patient and the potential waste of organs. However, the initial statement in the first paragraph suggests that this policy should be reevaluated due to advances in HIV management. The information does not explicitly state that *all* HIV-infected patients with end-stage renal disease should be excluded, but it indicates a strong reluctance among transplant centers to perform such transplants. Therefore, the information does not support the query.\n",
            "\n",
            "no\n",
            "\n",
            "-------------------------\n",
            "\n",
            "YES/NO: no\n",
            "\n",
            "GT ANSWER: no\n",
            "\n",
            "GT LONG ANSWER:\n",
            "-------------------------\n",
            "The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "query = sample_elm['question']\n",
        "\n",
        "response = query_engine.query(query)\n",
        "\n",
        "delim = \"\".join([\"-\"]*25)\n",
        "print(f'QUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1808e94-337b-4e4b-9637-2b79746ddddc",
      "metadata": {
        "id": "a1808e94-337b-4e4b-9637-2b79746ddddc"
      },
      "source": [
        "#### [OPTIONAL] [Ragas](https://docs.ragas.io/en/latest/) evaluation\n",
        "Following are the commonly used metrics for evaluating a RAG workflow:\n",
        "* [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/): Measures the factual correctness of the generated answer based on the retrived context. Value lies between 0 and 1. **Evaluated using a LLM.**\n",
        "* [Answer Relevance](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/answer_relevance/): Measures how relevant the answer is to the given query. Value lies between 0 and 1. **Evaluated using a LLM.**\n",
        "* [Context Precision](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/): Precision of the retriever as measured using the retrieved and the ground truth context. Value lies between 0 and 1. LLM can be used for evaluation.\n",
        "* [Context Recall](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_recall/): Recall of the retriever as measured using the retrieved and the ground truth context. Value lies between 0 and 1. LLM can be used for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14877b8-e764-4e1b-b450-c29381567d44",
      "metadata": {
        "id": "c14877b8-e764-4e1b-b450-c29381567d44"
      },
      "source": [
        "Note: If you are planning to use **OpenAI models as evaluation LLMs**, store your OpenAI API key in ```~/.ragas_openai.env``` using the following format:\n",
        "\n",
        "```bash\n",
        "   export RAGAS_OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n",
        "   export RAGAS_OPENAI_API_KEY=<openai_api_key>\n",
        "```\n",
        "\n",
        "Once done, **uncomment the next cell** to load these environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb7ac22-3b60-465f-9226-f0db639ddf2a",
      "metadata": {
        "id": "4eb7ac22-3b60-465f-9226-f0db639ddf2a"
      },
      "outputs": [],
      "source": [
        "# from utils.load_secrets import load_env_file_ragas\n",
        "# load_env_file_ragas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng RagasEval from utils.rag_utils.py - update to include support for Vertex AI Gemini*\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "\n",
        "from ragas import EvaluationDataset, evaluate as ragas_evaluate\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper, LlamaIndexEmbeddingsWrapper\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    NonLLMContextPrecisionWithReference,\n",
        "    NonLLMContextRecall,\n",
        "    ResponseRelevancy,\n",
        ")\n",
        "\n",
        "RAGAS_METRIC_MAP = {\n",
        "    \"faithfulness\": Faithfulness(),\n",
        "    \"relevancy\": ResponseRelevancy(),\n",
        "    \"recall\": NonLLMContextRecall(),\n",
        "    \"precision\": NonLLMContextPrecisionWithReference(),\n",
        "}\n",
        "\n",
        "class RagasEval:\n",
        "    def __init__(\n",
        "        self, metrics, eval_llm_type, eval_llm_name, embed_model_type, embed_model_name, **kwargs\n",
        "    ):\n",
        "        self.eval_llm_type = eval_llm_type  # \"openai\", \"cohere\", \"local\", \"kscope\", \"vertex\"\n",
        "        self.eval_llm_name = eval_llm_name\n",
        "\n",
        "        self.temperature = kwargs.get(\"temperature\", 0.0)\n",
        "        self.max_tokens = kwargs.get(\"max_tokens\", 256)\n",
        "\n",
        "        self.embed_model_type = embed_model_type # \"openai\", \"vertex\", \"vertex-endpoint\"\n",
        "        self.embed_model_name = embed_model_name\n",
        "        self.embed_model_endpoint_id = kwargs.get(\"embed_model_endpoint_id\", None)\n",
        "        self.embed_model_use_dedicated_endpoint = kwargs.get(\"embed_model_use_dedicated_endpoint\", False)\n",
        "\n",
        "        self._prepare_embedding()\n",
        "        self._prepare_llm()\n",
        "\n",
        "        self.metrics = [RAGAS_METRIC_MAP[elm] for elm in metrics]\n",
        "\n",
        "    def _prepare_data(self, data):\n",
        "        return EvaluationDataset.from_list(data)\n",
        "\n",
        "    def _prepare_embedding(self):\n",
        "        model_kwargs = {\"device\": \"cuda\", \"trust_remote_code\": True}\n",
        "        encode_kwargs = {\n",
        "            \"normalize_embeddings\": True\n",
        "        }  # set True to compute cosine similarity\n",
        "\n",
        "        if self.embed_model_type == \"openai\":\n",
        "          self.eval_embedding = LangchainEmbeddingsWrapper(\n",
        "              HuggingFaceEmbeddings(\n",
        "                model_name=self.embed_model_name,\n",
        "                model_kwargs=model_kwargs,\n",
        "                encode_kwargs=encode_kwargs,\n",
        "              )\n",
        "          )\n",
        "        elif self.embed_model_type == \"vertex\":\n",
        "          self.eval_embedding = LangchainEmbeddingsWrapper(\n",
        "              VertexAIEmbeddings(\n",
        "                  model_name=self.embed_model_name,\n",
        "                  credentials=credentials,\n",
        "              )\n",
        "          )\n",
        "        elif self.embed_model_type == \"vertex-endpoint\":\n",
        "          self.eval_embedding = LlamaIndexEmbeddingsWrapper(\n",
        "              VertexEndpointEmbedding(\n",
        "                endpoint_id=self.embed_model_endpoint_id,\n",
        "                project_id=PROJECT_ID,\n",
        "                location=REGION,\n",
        "                endpoint_kwargs={\n",
        "                    \"use_dedicated_endpoint\": self.embed_model_use_dedicated_endpoint,\n",
        "                },\n",
        "              )\n",
        "          )\n",
        "\n",
        "    def _prepare_llm(self):\n",
        "        if self.eval_llm_type == \"local\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                HuggingFaceEndpoint(\n",
        "                    repo_id=f\"meta-llama/{self.eval_llm_name}\",\n",
        "                    temperautre=self.temperature,\n",
        "                    max_new_tokens=self.max_tokens,\n",
        "                    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"kscope\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatOpenAI(\n",
        "                    model=self.eval_llm_name,\n",
        "                    temperature=self.temperature,\n",
        "                    max_tokens=self.max_tokens,\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"openai\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatOpenAI(\n",
        "                    model=self.eval_llm_name,\n",
        "                    temperature=self.temperature,\n",
        "                    max_tokens=self.max_tokens,\n",
        "                    base_url=os.environ[\"RAGAS_OPENAI_BASE_URL\"],\n",
        "                    api_key=os.environ[\"RAGAS_OPENAI_API_KEY\"],\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"cohere\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatCohere(\n",
        "                    model=self.eval_llm_name,\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"vertex\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatVertexAI(\n",
        "                  model_name=self.eval_llm_name,\n",
        "                  temperature=self.temperature,\n",
        "                  max_tokens=self.max_tokens,\n",
        "              )\n",
        "            )\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        eval_data = self._prepare_data(data)\n",
        "        result = ragas_evaluate(\n",
        "            dataset=eval_data,\n",
        "            metrics=self.metrics,\n",
        "            llm=self.eval_llm,\n",
        "            embeddings=self.eval_embedding,\n",
        "        )\n",
        "        return result"
      ],
      "metadata": {
        "id": "EHIQCTgqQHdM",
        "cellView": "form"
      },
      "id": "EHIQCTgqQHdM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdb6372-9364-4bd3-9b81-bf403f0bc006",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bdb6372-9364-4bd3-9b81-bf403f0bc006",
        "outputId": "3e372df0-5591-4127-89b9-3f6aebe188be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'reference': 'The great majority of U.S. renal transplant centers will not '\n",
            "               'transplant kidneys to HIV-infected patients with end-stage '\n",
            "               'renal disease, even if their infection is asymptomatic. '\n",
            "               'However, advances in the management of HIV infection and a '\n",
            "               'review of relevant ethical issues suggest that this approach '\n",
            "               'should be reconsidered.',\n",
            "  'reference_contexts': ['Human immunodeficiency virus (HIV)-infected patients '\n",
            "                         'have generally been excluded from transplantation. '\n",
            "                         'Recent advances in the management and prognosis of '\n",
            "                         'these patients suggest that this policy should be '\n",
            "                         'reevaluated. To explore the current views of U.S. '\n",
            "                         'transplant centers toward transplanting asymptomatic '\n",
            "                         'HIV-infected patients with end-stage renal disease, '\n",
            "                         'a written survey was mailed to the directors of '\n",
            "                         'transplantation at all 248 renal transplant centers '\n",
            "                         'in the United States. All 148 responding centers '\n",
            "                         'said they require HIV testing of prospective kidney '\n",
            "                         'recipients, and 84% of these centers would not '\n",
            "                         'transplant an individual who refuses HIV testing. '\n",
            "                         'The vast majority of responding centers would not '\n",
            "                         'transplant a kidney from a cadaveric (88%) or a '\n",
            "                         'living donor (91%) into an asymptomatic HIV-infected '\n",
            "                         'patient who is otherwise a good candidate for '\n",
            "                         'transplantation. Among the few centers that would '\n",
            "                         'consider transplanting an HIV-infected patient, not '\n",
            "                         'a single center had performed such a transplant in '\n",
            "                         'the year prior to the survey. Most centers fear that '\n",
            "                         'transplantation in the face of HIV infection would '\n",
            "                         'be harmful to the individual, and some believe that '\n",
            "                         'it would be a waste of precious organs.'],\n",
            "  'response': 'Based on the provided context, the majority of U.S. transplant '\n",
            "              'centers surveyed would not transplant a kidney into an '\n",
            "              'asymptomatic HIV-infected patient. The primary reasons cited '\n",
            "              'are concerns about harm to the patient and the potential waste '\n",
            "              'of organs. However, the initial statement in the first '\n",
            "              'paragraph suggests that this policy should be reevaluated due '\n",
            "              'to advances in HIV management. The information does not '\n",
            "              'explicitly state that *all* HIV-infected patients with '\n",
            "              'end-stage renal disease should be excluded, but it indicates a '\n",
            "              'strong reluctance among transplant centers to perform such '\n",
            "              'transplants. Therefore, the information does not support the '\n",
            "              'query.\\n'\n",
            "              '\\n'\n",
            "              'no\\n',\n",
            "  'retrieved_contexts': ['Human immunodeficiency virus (HIV)-infected patients '\n",
            "                         'have generally been excluded from transplantation. '\n",
            "                         'Recent advances in the management and prognosis of '\n",
            "                         'these patients suggest that this policy should be '\n",
            "                         'reevaluated. To explore the current views of U.S. '\n",
            "                         'transplant centers toward transplanting asymptomatic '\n",
            "                         'HIV-infected patients with end-stage renal disease, '\n",
            "                         'a written survey was mailed to the directors of '\n",
            "                         'transplantation at all 248 renal transplant centers '\n",
            "                         'in the United States. All 148 responding centers '\n",
            "                         'said they require HIV testing of prospective kidney '\n",
            "                         'recipients, and 84% of these centers would not '\n",
            "                         'transplant an individual who refuses HIV testing. '\n",
            "                         'The vast majority of responding centers would not '\n",
            "                         'transplant a kidney from a cadaveric (88%) or a '\n",
            "                         'living donor (91%) into an asymptomatic HIV-infected '\n",
            "                         'patient who is otherwise a good candidate for '\n",
            "                         'transplantation. Among the few centers that would '\n",
            "                         'consider transplanting an HIV-infected patient, not '\n",
            "                         'a single center had performed such a transplant in '\n",
            "                         'the year prior to the survey. Most centers fear that '\n",
            "                         'transplantation in the face of HIV infection would '\n",
            "                         'be harmful to the individual, and some believe that '\n",
            "                         'it would be a waste of precious organs.',\n",
            "                         'Kidneys from elderly donors tend to be implanted in '\n",
            "                         'recipients who are also elderly. We present the '\n",
            "                         'results obtained after 10 years of evolution on '\n",
            "                         'transplanting elderly kidneys into young recipients. '\n",
            "                         'Ninety-one consecutive transplants are studied, '\n",
            "                         'carried out in our center with kidneys from cadaver '\n",
            "                         'donors older than 60 years implanted in recipients '\n",
            "                         'younger than 60 years. The control group is made up '\n",
            "                         'of 91 transplants, matched with those from the study '\n",
            "                         'group, whose donor and recipient were younger than '\n",
            "                         '60 years. There were no differences between groups '\n",
            "                         'with regard to recipient age, sex, cause of death '\n",
            "                         'and renal function of the donor, hepatitis C and '\n",
            "                         'cytomegalovirus serologies, cold ischemia time, '\n",
            "                         'tubular necrosis, immediate diuresis, need for '\n",
            "                         'dialysis, human leukocyte antigen incompatibilities, '\n",
            "                         'hypersensitized patients, acute rejection, waiting '\n",
            "                         'time on dialysis, and days of admission. Survival in '\n",
            "                         'both groups at 1, 5, and 10 years was 97.6%, 87.2%, '\n",
            "                         'and 76.6% vs.',\n",
            "                         'Recently, there has been increasing interest in the '\n",
            "                         'role of \"treatment as prevention\" (TasP). Some of '\n",
            "                         'the questions regarding TasP strategies arise from '\n",
            "                         'the perceived difficulties in achieving and '\n",
            "                         'maintaining viral load (VL) suppression over time '\n",
            "                         'and the risk of emergence of viral resistance that '\n",
            "                         'could compromise future treatment options. This '\n",
            "                         'study was conducted to assess these questions in a '\n",
            "                         'resource-limited setting. We performed a '\n",
            "                         'retrospective observational study of HIV-infected '\n",
            "                         'patients diagnosed in the pre-HAART era on follow-up '\n",
            "                         'at a private center from Buenos Aires, Argentina. '\n",
            "                         'Socio-demographic, clinical, and laboratory data '\n",
            "                         'were extracted from clinical charts. Analyses were '\n",
            "                         'performed to test for potential associations of '\n",
            "                         'selected variables with current virologic failure or '\n",
            "                         'use of third-line drugs. Of 619 patients on '\n",
            "                         'follow-up, 82 (13.2%) were diagnosed in the '\n",
            "                         'pre-HAART era. At the time of our study, 79 (96.3%) '\n",
            "                         'patients were on HAART, with a median duration of 14 '\n",
            "                         'years (IQR 12-15) of therapy, and exposure to mono '\n",
            "                         'or dual nucleoside reverse transcriptase inhibitors '\n",
            "                         'regimens in 47.8% of cases.',\n",
            "                         'Recent studies have shown that early antiretroviral '\n",
            "                         'therapy (ART) initiation results in significant HIV '\n",
            "                         'transmission reduction. This is the rationale behind '\n",
            "                         'the \"test and treat\" policy of the World Health '\n",
            "                         'Organization (WHO). Implementation of this policy '\n",
            "                         'will lead to an increased incidence of ART-related '\n",
            "                         'adverse effects, especially in sub-Saharan Africa '\n",
            "                         '(SSA). Is the region yet ready to cope with such a '\n",
            "                         'challenging issue? The introduction and widespread '\n",
            "                         'use of ART have drastically changed the natural '\n",
            "                         'history of HIV/AIDS, but exposure to ART leads to '\n",
            "                         'serious medication-related adverse effects mainly '\n",
            "                         'explained by mitochondrial toxicities, and the '\n",
            "                         'situation will get worse in the near future. Indeed, '\n",
            "                         'ART is associated with an increased risk of '\n",
            "                         'developing cardiovascular disease, lipodystrophy, '\n",
            "                         'prediabetes and overt diabetes, insulin resistance '\n",
            "                         'and hyperlactatemia/lactic acidosis. The prevalence '\n",
            "                         'of these disorders is already high in SSA, and the '\n",
            "                         'situation will be exacerbated by the implementation '\n",
            "                         'of the new WHO recommendations.',\n",
            "                         'A higher prevalence of cardiovascular risk factors '\n",
            "                         '(CRFs) in HIV-infected patients, together with '\n",
            "                         'chronic infection and treatments, has resulted in an '\n",
            "                         'increased risk of silent myocardial ischaemia (SMI). '\n",
            "                         'The objective of this study was to evaluate whether '\n",
            "                         'myocardial SPECT should be used for screening '\n",
            "                         'HIV-infected patients with no clinical symptoms of '\n",
            "                         'coronary artery disease. The prevalence of SMI '\n",
            "                         'detected by myocardial SPECT was determined in 94 '\n",
            "                         'HIV-infected patients with a normal clinical '\n",
            "                         'cardiovascular examination in relation to '\n",
            "                         'anthropomorphic parameters, CRFs, inflammatory and '\n",
            "                         'HIV infection status, and treatment. Coronary artery '\n",
            "                         'disease was detected in nine patients (eight with '\n",
            "                         'ischaemia, one with myocardial infarction), '\n",
            "                         'corresponding to 9.6 % positivity. All but two of '\n",
            "                         'the scintigraphic diagnoses of ischaemia were '\n",
            "                         'confirmed by coronarography. Univariate analysis '\n",
            "                         'revealed that the overall number of CRFs and the '\n",
            "                         'combination of gender and age were associated with a '\n",
            "                         'diagnosis of SMI (p<0.05). According to multivariate '\n",
            "                         'analysis, the only independent parameter '\n",
            "                         'significantly associated with the scintigraphic '\n",
            "                         'diagnosis of SMI was the combination of gender and '\n",
            "                         'age (p = 0.01).'],\n",
            "  'user_input': 'Should all human immunodeficiency virus-infected patients '\n",
            "                'with end-stage renal disease be excluded from '\n",
            "                'transplantation?'}]\n"
          ]
        }
      ],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "pprint(eval_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0582ac-b840-457d-86eb-4aa9d8fc43de",
      "metadata": {
        "id": "ed0582ac-b840-457d-86eb-4aa9d8fc43de"
      },
      "outputs": [],
      "source": [
        "eval_obj = RagasEval(\n",
        "    metrics=[\"faithfulness\", \"relevancy\", \"recall\", \"precision\"],\n",
        "    max_tokens=1024,\n",
        "    **rag_cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d26888-6b7b-4a41-9dac-9c242554a136",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "055a661c05314f148dd4d695ed437560",
            "2548d64e26bf4af3832af8ca1a19290a",
            "26c67a8ba1e3470497afb8195393655d",
            "ac014274dbae44a9bbccb77ec82b532c",
            "2926218039504b5abb6128e74c041673",
            "e77c088092bc4f71b685fa5b6f64dbdb",
            "f2b6eddc34874c6e9cf4ea2a921867bc",
            "8239698d9c1144be9301e6ca2f6bc3fb",
            "3401d979fc534136b9cc8ef7564d318d",
            "895490b7bc4d4e3ba33f376bfd0279f0",
            "e0259d69288146abb65d3ac9711a31fc"
          ]
        },
        "id": "65d26888-6b7b-4a41-9dac-9c242554a136",
        "outputId": "bca7ddc7-7061-43d1-b289-a90c7ae9117d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "055a661c05314f148dd4d695ed437560"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': 0.8571, 'answer_relevancy': 0.0000, 'non_llm_context_recall': 1.0000, 'non_llm_context_precision_with_reference': 1.0000}\n"
          ]
        }
      ],
      "source": [
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe5bcc7-900b-4939-aa71-65385b5c765c",
      "metadata": {
        "id": "dfe5bcc7-900b-4939-aa71-65385b5c765c"
      },
      "source": [
        "### 5.1 - Dense Search\n",
        "Set *hybrid_search_alpha* to 1.0 for dense vector search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131eaa05-ae98-481c-9967-cc6c74b9d383",
      "metadata": {
        "id": "131eaa05-ae98-481c-9967-cc6c74b9d383"
      },
      "outputs": [],
      "source": [
        "rag_cfg[\"hybrid_search_alpha\"] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fba7d70-0bd7-42b7-ba52-f1924e96b338",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fba7d70-0bd7-42b7-ba52-f1924e96b338",
        "outputId": "06125dcd-8305-4155-f19b-fae8f502d9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 1.0,\n",
            " 'query_mode': 'default',\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': False}\n",
            "\n",
            "\n",
            "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
            "\n",
            "RESPONSE:\n",
            "-------------------------\n",
            "Based on the provided context, the majority of U.S. transplant centers surveyed would not transplant a kidney into an asymptomatic HIV-infected patient. The primary reasons cited are concerns about harm to the patient and the potential waste of organs. However, the initial statement in the first paragraph suggests that this policy should be reevaluated due to advances in HIV management. The information does not explicitly state that *all* HIV-infected patients with end-stage renal disease should be excluded, but it indicates a strong reluctance among transplant centers to perform such transplants. Therefore, the information does not support the query.\n",
            "\n",
            "no\n",
            "\n",
            "-------------------------\n",
            "\n",
            "YES/NO: no\n",
            "\n",
            "GT ANSWER: no\n",
            "\n",
            "GT LONG ANSWER:\n",
            "-------------------------\n",
            "The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "# Recreate query engine\n",
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)\n",
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index\n",
        ").create(**query_engine_args)\n",
        "\n",
        "# Get response\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Print response\n",
        "print(f'\\n\\nQUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "893b4da5-2391-49ca-bbe6-ed7f2433faa5",
      "metadata": {
        "id": "893b4da5-2391-49ca-bbe6-ed7f2433faa5"
      },
      "source": [
        "#### [OPTIONAL] Ragas evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b240d3-2b09-4c28-8c20-6d9be47dc963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "88de5ddb8e144e4090c2d05836b3ee04",
            "3d0439584a734bec8788464e807f2088",
            "7d970a3359874445a7b9121533c884ae",
            "34c0725fb34749f296e24dcf3a3d52bc",
            "18159e5a9e59488ea0bb8a226363b73e",
            "a68d3aa745694c4893d476b9a50f079c",
            "1ae1efbd2f754f948ae153fe4114a5d2",
            "d6480f7821ad41c69bdd99c29a1878d0",
            "a908b62b8b3d4b35bfd93c841d11b1c0",
            "61d9be4418ff49869736caf8609a91d8",
            "c050c00bbe1342a9af32468f0638e2e0"
          ]
        },
        "id": "78b240d3-2b09-4c28-8c20-6d9be47dc963",
        "outputId": "3b6aaa1f-4e51-4371-e648-f7513ed41396"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88de5ddb8e144e4090c2d05836b3ee04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': 0.8571, 'answer_relevancy': 0.0000, 'non_llm_context_recall': 1.0000, 'non_llm_context_precision_with_reference': 1.0000}\n"
          ]
        }
      ],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "\n",
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69911c71-1639-4ed8-86cb-85abbbc15467",
      "metadata": {
        "id": "69911c71-1639-4ed8-86cb-85abbbc15467"
      },
      "source": [
        "### 5.2 - Hybrid Search\n",
        "Set *hybrid_search_alpha* to 0.5 for hybrid search with equal weightage for dense and sparse (keyword-based) search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a526b44-5473-4850-9209-7f9b9446f5b3",
      "metadata": {
        "id": "6a526b44-5473-4850-9209-7f9b9446f5b3"
      },
      "outputs": [],
      "source": [
        "rag_cfg[\"hybrid_search_alpha\"] = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf513126-01fe-4f6f-b18a-b6c5224f07e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf513126-01fe-4f6f-b18a-b6c5224f07e0",
        "outputId": "d306a9d2-05da-43cf-e65c-ea1671a81708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 0.5,\n",
            " 'query_mode': 'default',\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': False}\n",
            "\n",
            "\n",
            "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
            "\n",
            "RESPONSE:\n",
            "-------------------------\n",
            "Based on the provided context, the majority of U.S. transplant centers surveyed would not transplant a kidney into an asymptomatic HIV-infected patient. The primary reasons cited are concerns about harm to the patient and the potential waste of organs. However, the initial statement in the first paragraph suggests that this policy should be reevaluated due to advances in HIV management. The information does not explicitly state that *all* HIV-infected patients with end-stage renal disease should be excluded, but it indicates a strong reluctance among transplant centers to perform such transplants. Therefore, the information does not support the query.\n",
            "\n",
            "no\n",
            "\n",
            "-------------------------\n",
            "\n",
            "YES/NO: no\n",
            "\n",
            "GT ANSWER: no\n",
            "\n",
            "GT LONG ANSWER:\n",
            "-------------------------\n",
            "The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "# Recreate query engine\n",
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)\n",
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index\n",
        ").create(**query_engine_args)\n",
        "\n",
        "# Get response\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Print response\n",
        "print(f'\\n\\nQUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028e7abc-1fbd-4d66-a724-dddba4733721",
      "metadata": {
        "id": "028e7abc-1fbd-4d66-a724-dddba4733721"
      },
      "source": [
        "#### [OPTIONAL] Ragas evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03197082-eb44-46d7-b982-816f8811560f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "a5bb2c6d974548d881397e3ee22a9d04",
            "91af5ab13e1240298bc925d4a2dcec44",
            "244c064b24c54cdba6448e781278132c",
            "9986cf1497744927ad5bb92d3e1e1c62",
            "a0191d941edc40c5a4e795f814a16d8c",
            "38923aee63284278aba7cb52abd91726",
            "673a06b7e2934aa1a88b860ce56f6dac",
            "ce48605760e84d6f99acac6838fcf32a",
            "16885d5ff40c4797bb105f18bb8f9351",
            "3f6ef79727504110bbd90c130601b625",
            "a80f51d00f474916ab94d5c230bc0f93"
          ]
        },
        "id": "03197082-eb44-46d7-b982-816f8811560f",
        "outputId": "e2b45e09-0f04-44ec-c76e-dc81979a0a70"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5bb2c6d974548d881397e3ee22a9d04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': 0.8571, 'answer_relevancy': 0.0000, 'non_llm_context_recall': 1.0000, 'non_llm_context_precision_with_reference': 1.0000}\n"
          ]
        }
      ],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "\n",
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ca6e00-2580-4d98-a832-3fd0f8094e1a",
      "metadata": {
        "id": "17ca6e00-2580-4d98-a832-3fd0f8094e1a"
      },
      "source": [
        "### 5.3 - Using Re-ranker\n",
        "Set *use_reranker* to *True* to re-rank the context after retrieving it from the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acad8e06-e9a1-4d3c-b6e5-fa8bc6b1b8a6",
      "metadata": {
        "id": "acad8e06-e9a1-4d3c-b6e5-fa8bc6b1b8a6"
      },
      "outputs": [],
      "source": [
        "rag_cfg[\"use_reranker\"] = True\n",
        "rag_cfg[\"hybrid_search_alpha\"] = 1.0 # Using dense search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c714b8f-45db-4ab6-9e28-7afa8c8a31ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "6c714b8f-45db-4ab6-9e28-7afa8c8a31ed",
        "outputId": "6619793d-4d44-48dd-adf2-c31be2140eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 1.0,\n",
            " 'query_mode': 'default',\n",
            " 'rerank_top_k': 3,\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': True}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResponseValidationError",
          "evalue": "The model response did not complete successfully.\nFinish reason: 2.\nFinish message: .\nSafety ratings: [].\nTo protect the integrity of the chat session, the request and response were not added to chat history.\nTo skip the response validation, specify `model.start_chat(response_validation=False)`.\nNote that letting blocked or otherwise incomplete responses into chat history might lead to future interactions being blocked by the service.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResponseValidationError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-22315a484dff>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Print response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_query_engine.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mstr_or_query_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mquery_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         dispatcher.event(\n\u001b[1;32m     54\u001b[0m             \u001b[0mQueryEndEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mCBEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY_STR\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         ) as query_event:\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             response = self._response_synthesizer.synthesize(\n\u001b[1;32m    180\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNodeWithScore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_node_postprocessors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNodeWithScore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/query_engine/retriever_query_engine.py\u001b[0m in \u001b[0;36m_apply_node_postprocessors\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m    125\u001b[0m     ) -> List[NodeWithScore]:\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_postprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_postprocessors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             nodes = node_postprocessor.postprocess_nodes(\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/postprocessor/types.py\u001b[0m in \u001b[0;36mpostprocess_nodes\u001b[0;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postprocess_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/postprocessor/llm_rerank.py\u001b[0m in \u001b[0;36m_postprocess_nodes\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mfmt_batch_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_node_batch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# call each batch independently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             raw_response = self.llm.predict(\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice_select_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mcontext_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt_batch_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_chat_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprompt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mchat_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0;31m# If the result is a Future, wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/callbacks.py\u001b[0m in \u001b[0;36mwrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 )\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mf_return_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     callback_manager.on_event_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/vertex/base.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m             )\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         generation = completion_with_retry(\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chat_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/vertex/utils.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(client, prompt, max_retries, chat, stream, is_gemini, params, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_retry_check_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RetryCallState\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/vertex/utils.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mgeneration_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             return generation.send_message(\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, content, generation_config, safety_settings, tools, labels, stream)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m             return self._send_message(\n\u001b[0m\u001b[1;32m   1338\u001b[0m                 \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, content, generation_config, safety_settings, tools, labels)\u001b[0m\n\u001b[1;32m   1473\u001b[0m             \u001b[0;31m# By default we're not adding incomplete interactions to history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_validator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m                 self._response_validator(\n\u001b[0m\u001b[1;32m   1476\u001b[0m                     \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m                     \u001b[0mrequest_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36m_validate_response\u001b[0;34m(response, request_contents, response_chunks)\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;34m\"Note that letting blocked or otherwise incomplete responses into chat history might lead to future interactions being blocked by the service.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         )\n\u001b[0;32m-> 1237\u001b[0;31m         raise ResponseValidationError(\n\u001b[0m\u001b[1;32m   1238\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0mrequest_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_contents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResponseValidationError\u001b[0m: The model response did not complete successfully.\nFinish reason: 2.\nFinish message: .\nSafety ratings: [].\nTo protect the integrity of the chat session, the request and response were not added to chat history.\nTo skip the response validation, specify `model.start_chat(response_validation=False)`.\nNote that letting blocked or otherwise incomplete responses into chat history might lead to future interactions being blocked by the service."
          ]
        }
      ],
      "source": [
        "# Recreate query engine\n",
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)\n",
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index\n",
        ").create(**query_engine_args)\n",
        "\n",
        "# Get response\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Print response\n",
        "print(f'\\n\\nQUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6f4714-ee80-46ba-bf14-ec4762f70a08",
      "metadata": {
        "id": "2b6f4714-ee80-46ba-bf14-ec4762f70a08"
      },
      "source": [
        "#### [OPTIONAL] Ragas evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e43ba3a-6824-4246-8df9-3e9ce7efa5b1",
      "metadata": {
        "id": "8e43ba3a-6824-4246-8df9-3e9ce7efa5b1"
      },
      "outputs": [],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "\n",
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag_pubmed_qa",
      "language": "python",
      "name": "rag_pubmed_qa"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "name": "darmod_pubmed_qa_llamaindex_vertex_ai_model_garden.ipynb",
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "055a661c05314f148dd4d695ed437560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2548d64e26bf4af3832af8ca1a19290a",
              "IPY_MODEL_26c67a8ba1e3470497afb8195393655d",
              "IPY_MODEL_ac014274dbae44a9bbccb77ec82b532c"
            ],
            "layout": "IPY_MODEL_2926218039504b5abb6128e74c041673"
          }
        },
        "2548d64e26bf4af3832af8ca1a19290a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e77c088092bc4f71b685fa5b6f64dbdb",
            "placeholder": "​",
            "style": "IPY_MODEL_f2b6eddc34874c6e9cf4ea2a921867bc",
            "value": "Evaluating: 100%"
          }
        },
        "26c67a8ba1e3470497afb8195393655d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8239698d9c1144be9301e6ca2f6bc3fb",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3401d979fc534136b9cc8ef7564d318d",
            "value": 4
          }
        },
        "ac014274dbae44a9bbccb77ec82b532c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895490b7bc4d4e3ba33f376bfd0279f0",
            "placeholder": "​",
            "style": "IPY_MODEL_e0259d69288146abb65d3ac9711a31fc",
            "value": " 4/4 [00:03&lt;00:00,  1.14s/it]"
          }
        },
        "2926218039504b5abb6128e74c041673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e77c088092bc4f71b685fa5b6f64dbdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2b6eddc34874c6e9cf4ea2a921867bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8239698d9c1144be9301e6ca2f6bc3fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3401d979fc534136b9cc8ef7564d318d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "895490b7bc4d4e3ba33f376bfd0279f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0259d69288146abb65d3ac9711a31fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88de5ddb8e144e4090c2d05836b3ee04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d0439584a734bec8788464e807f2088",
              "IPY_MODEL_7d970a3359874445a7b9121533c884ae",
              "IPY_MODEL_34c0725fb34749f296e24dcf3a3d52bc"
            ],
            "layout": "IPY_MODEL_18159e5a9e59488ea0bb8a226363b73e"
          }
        },
        "3d0439584a734bec8788464e807f2088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68d3aa745694c4893d476b9a50f079c",
            "placeholder": "​",
            "style": "IPY_MODEL_1ae1efbd2f754f948ae153fe4114a5d2",
            "value": "Evaluating: 100%"
          }
        },
        "7d970a3359874445a7b9121533c884ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6480f7821ad41c69bdd99c29a1878d0",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a908b62b8b3d4b35bfd93c841d11b1c0",
            "value": 4
          }
        },
        "34c0725fb34749f296e24dcf3a3d52bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61d9be4418ff49869736caf8609a91d8",
            "placeholder": "​",
            "style": "IPY_MODEL_c050c00bbe1342a9af32468f0638e2e0",
            "value": " 4/4 [00:04&lt;00:00,  1.21s/it]"
          }
        },
        "18159e5a9e59488ea0bb8a226363b73e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a68d3aa745694c4893d476b9a50f079c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ae1efbd2f754f948ae153fe4114a5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6480f7821ad41c69bdd99c29a1878d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a908b62b8b3d4b35bfd93c841d11b1c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61d9be4418ff49869736caf8609a91d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c050c00bbe1342a9af32468f0638e2e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5bb2c6d974548d881397e3ee22a9d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91af5ab13e1240298bc925d4a2dcec44",
              "IPY_MODEL_244c064b24c54cdba6448e781278132c",
              "IPY_MODEL_9986cf1497744927ad5bb92d3e1e1c62"
            ],
            "layout": "IPY_MODEL_a0191d941edc40c5a4e795f814a16d8c"
          }
        },
        "91af5ab13e1240298bc925d4a2dcec44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38923aee63284278aba7cb52abd91726",
            "placeholder": "​",
            "style": "IPY_MODEL_673a06b7e2934aa1a88b860ce56f6dac",
            "value": "Evaluating: 100%"
          }
        },
        "244c064b24c54cdba6448e781278132c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce48605760e84d6f99acac6838fcf32a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16885d5ff40c4797bb105f18bb8f9351",
            "value": 4
          }
        },
        "9986cf1497744927ad5bb92d3e1e1c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f6ef79727504110bbd90c130601b625",
            "placeholder": "​",
            "style": "IPY_MODEL_a80f51d00f474916ab94d5c230bc0f93",
            "value": " 4/4 [00:03&lt;00:00,  1.14s/it]"
          }
        },
        "a0191d941edc40c5a4e795f814a16d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38923aee63284278aba7cb52abd91726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "673a06b7e2934aa1a88b860ce56f6dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce48605760e84d6f99acac6838fcf32a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16885d5ff40c4797bb105f18bb8f9351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f6ef79727504110bbd90c130601b625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a80f51d00f474916ab94d5c230bc0f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}