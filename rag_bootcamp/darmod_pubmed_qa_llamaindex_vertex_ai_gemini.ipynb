{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e979eb9-3fe0-49c2-b814-f4025ed81934",
      "metadata": {
        "id": "2e979eb9-3fe0-49c2-b814-f4025ed81934"
      },
      "source": [
        "# PubMed QA using LlamaIndex\n",
        "\n",
        "## Introduction\n",
        "This notebook presents a RAG workflow for the [PubMed QA](https://pubmedqa.github.io/) task using [LlamaIndex](https://www.llamaindex.ai/). The code is written in a configurable fashion, giving you the flexibility to edit the RAG configuration and observe the change in output/responses.\n",
        "\n",
        "It covers a step-by-step procedure for building the RAG workflow (Stages 1-4) and later runs the pipeline on a sample from the dataset. The notebook also covers the sparse, dense, hybrid retrieval strategies along with the re-ranker. We have alse added an optional component for RAG evaluation using the [Ragas](https://docs.ragas.io/en/stable/) library.\n",
        "\n",
        "### <u>Requirements</u>\n",
        "1. As you will accessing the LLMs and embedding models through Vector AI Engineering's Kaleidoscope Service (Vector Inference + Autoscaling), you will need to request a KScope API Key:\n",
        "\n",
        "      Run the following command (replace ```<user_id>``` and ```<password>```) from **within the cluster** to obtain the API Key. The ```access_token``` in the output is your KScope API Key.\n",
        "  ```bash\n",
        "  curl -X POST -d \"grant_type=password\" -d \"username=<user_id>\" -d \"password=<password>\" https://kscope.vectorinstitute.ai/token\n",
        "  ```\n",
        "2. After obtaining the `.env` configurations, make sure to create the ```.kscope.env``` file in your home directory (```/h/<user_id>```) and set the following env variables:\n",
        "- For local models through Kaleidoscope (KScope):\n",
        "    ```bash\n",
        "    export OPENAI_BASE_URL=\"https://kscope.vectorinstitute.ai/v1\"\n",
        "    export OPENAI_API_KEY=<kscope_api_key>\n",
        "    ```\n",
        "- For OpenAI models:\n",
        "   ```bash\n",
        "   export OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n",
        "   export OPENAI_API_KEY=<openai_api_key>\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e83bf1-b093-4acd-ad37-27bd56f5b8e4",
      "metadata": {
        "id": "d6e83bf1-b093-4acd-ad37-27bd56f5b8e4"
      },
      "source": [
        "## STAGE 0 - Set up the RAG workflow environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4e45a6-1ed6-4e90-b0b7-913566652b40",
      "metadata": {
        "id": "2d4e45a6-1ed6-4e90-b0b7-913566652b40"
      },
      "source": [
        "#### Import libraries, custom classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet \\\n",
        "  llama-index \\\n",
        "  google-cloud-secret-manager \\\n",
        "  datasets \\\n",
        "  llama-index-readers-json \\\n",
        "  llama-index-readers-file \\\n",
        "  llama-index-readers-gcs \\\n",
        "  llama-index-embeddings-vertex \\\n",
        "  llama-index-embeddings-google-genai \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-embeddings-text-embeddings-inference \\\n",
        "  llama-index-embeddings-vertex-endpoint \\\n",
        "  llama-index-llms-huggingface \\\n",
        "  llama-index-llms-openai-like \\\n",
        "  llama-index-llms-vertex \\\n",
        "  llama-index-llms-google-genai \\\n",
        "  faiss-cpu \\\n",
        "  llama-index-vector-stores-faiss \\\n",
        "  llama-index-vector-stores-weaviate \\\n",
        "  llama-index-vector-stores-vertexaivectorsearch \\\n",
        "  llama-index-retrievers-bm25 \\\n",
        "  rapidfuzz \\\n",
        "  ragas \\\n",
        "  pydantic>=2.10.4 \\\n",
        "  google-cloud-aiplatform>=1.76 \\\n",
        "  langchain-core \\\n",
        "  langchain-cohere \\\n",
        "  langchain-huggingface \\\n",
        "  langchain-google-vertexai\n",
        "# jkwng: restart the kernel after this"
      ],
      "metadata": {
        "id": "-qhpaWUvWKzA"
      },
      "id": "-qhpaWUvWKzA",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c1909a34-2063-4195-a90e-fd35ce1b5df5",
      "metadata": {
        "id": "c1909a34-2063-4195-a90e-fd35ce1b5df5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "82038178-2699-403b-b32a-342b8d19ae98",
      "metadata": {
        "id": "82038178-2699-403b-b32a-342b8d19ae98"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "from llama_index.core import ServiceContext, Settings, set_global_handler\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "\n",
        "# jkwng: in order to make this notebook self contained, i just cut and paste these into the notebook\n",
        "# from task_dataset import PubMedQATaskDataset\n",
        "\n",
        "# from utils.hosting_utils import RAGLLM\n",
        "# from utils.rag_utils import (\n",
        "#     DocumentReader, RAGEmbedding, RAGQueryEngine, RagasEval,\n",
        "#     extract_yes_no, validate_rag_cfg\n",
        "#     )\n",
        "# from utils.storage_utils import RAGIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23409b3f-d436-469f-90eb-6ff4a7ce9c92",
      "metadata": {
        "id": "23409b3f-d436-469f-90eb-6ff4a7ce9c92"
      },
      "source": [
        "#### Load config files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: we don't need this cell*"
      ],
      "metadata": {
        "id": "qoKgmcb64nPK"
      },
      "id": "qoKgmcb64nPK"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "94487a03-5c16-4683-b10b-026171b8b37d",
      "metadata": {
        "id": "94487a03-5c16-4683-b10b-026171b8b37d"
      },
      "outputs": [],
      "source": [
        "# Add root folder of the rag_bootcamp repo to PYTHONPATH\n",
        "current_dir = Path().resolve()\n",
        "parent_dir = current_dir.parent\n",
        "sys.path.insert(0, str(parent_dir))\n",
        "\n",
        "\n",
        "# from utils.load_secrets import load_env_file\n",
        "# load_env_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: or this cell*"
      ],
      "metadata": {
        "id": "7RZnoMQF4toP"
      },
      "id": "7RZnoMQF4toP"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "24a2b082-8f20-4567-9e37-69f0beaee9ef",
      "metadata": {
        "id": "24a2b082-8f20-4567-9e37-69f0beaee9ef"
      },
      "outputs": [],
      "source": [
        "# GENERATOR_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: however we need these variables specifically for deployment on Google Cloud*"
      ],
      "metadata": {
        "id": "WVABlJxFaYBL"
      },
      "id": "WVABlJxFaYBL"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "GCS_URI = \"jkwng-vertex-experiments/rag_bootcamp/pubmed_qa\""
      ],
      "metadata": {
        "id": "kio2HT3-abCx"
      },
      "id": "kio2HT3-abCx",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c70d889-6e10-45b7-be3b-52a5b56d4f43",
      "metadata": {
        "id": "8c70d889-6e10-45b7-be3b-52a5b56d4f43"
      },
      "source": [
        "#### Set RAG configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `text-embedding-005` for embeddings, Gemini 2.0 Flash for generation, and Gemini 2.0 Pro experimental for evaluation"
      ],
      "metadata": {
        "id": "cH-1XvGAiNIA"
      },
      "id": "cH-1XvGAiNIA"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_cfg = {\n",
        "    # Node parser config\n",
        "    \"chunk_size\": 256,\n",
        "    \"chunk_overlap\": 0,\n",
        "\n",
        "    # Embedding model config\n",
        "    # \"embed_model_type\": \"hf\",\n",
        "    # \"embed_model_name\": \"BAAI/bge-base-en-v1.5\",\n",
        "    \"embed_model_type\": \"vertex\",\n",
        "    \"embed_model_name\": \"text-embedding-005\",\n",
        "\n",
        "    # LLM config\n",
        "    # \"llm_type\": \"kscope\",\n",
        "    # \"llm_name\": \"Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"llm_type\": \"vertex\",\n",
        "    \"llm_name\": \"gemini-2.0-flash-001\",\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"temperature\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 50,\n",
        "    \"do_sample\": False,\n",
        "\n",
        "    # Vector DB config\n",
        "    \"vector_db_type\": \"weaviate\", # \"weaviate\"\n",
        "    #\"vector_db_type\": \"vertex\",\n",
        "    \"vector_db_name\": \"Pubmed_QA\",\n",
        "    # MODIFY THIS\n",
        "    \"weaviate_url\": \"https://ds4tx7ttr3ciaui5obmowg.c0.us-east1.gcp.weaviate.cloud\",\n",
        "\n",
        "    # Retriever and query config\n",
        "    \"retriever_type\": \"vector_index\", # \"vector_index\"\n",
        "    \"retriever_similarity_top_k\": 5,\n",
        "    \"query_mode\": \"default\", # \"default\", \"hybrid\" - jkwng: changed to default\n",
        "    \"hybrid_search_alpha\": 0.0, # float from 0.0 (sparse search - bm25) to 1.0 (vector search)\n",
        "    \"response_mode\": \"compact\",\n",
        "    \"use_reranker\": False,\n",
        "    \"rerank_top_k\": 3,\n",
        "\n",
        "    # Evaluation config\n",
        "    # \"eval_llm_type\": \"kscope\",\n",
        "    # \"eval_llm_name\": \"Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"eval_llm_type\": \"vertex\",\n",
        "    \"eval_llm_name\": \"gemini-2.0-pro-exp-02-05\"\n",
        "}"
      ],
      "metadata": {
        "id": "64LzeyZ7ipkD"
      },
      "id": "64LzeyZ7ipkD",
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7789ee5b-ceef-48c0-9e09-11ea2051233b",
      "metadata": {
        "id": "7789ee5b-ceef-48c0-9e09-11ea2051233b"
      },
      "source": [
        "#### Read Weaviate Key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: load weaviate API key from secret manager*"
      ],
      "metadata": {
        "id": "Ejm-SqmSJZ8J"
      },
      "id": "Ejm-SqmSJZ8J"
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "ed2f405b-4c51-4174-ae8b-03029adec4a8",
      "metadata": {
        "id": "ed2f405b-4c51-4174-ae8b-03029adec4a8"
      },
      "outputs": [],
      "source": [
        "from google.cloud import secretmanager\n",
        "\n",
        "client = secretmanager.SecretManagerServiceClient()\n",
        "\n",
        "# Access the secret\n",
        "name = f\"projects/{PROJECT_ID}/secrets/weaviate_key/versions/latest\"\n",
        "sm_response = client.access_secret_version(request={\"name\": name})\n",
        "\n",
        "# Extract and print the secret value\n",
        "weaviate_key = sm_response.payload.data.decode(\"UTF-8\")\n",
        "WEAVIATE_API_KEY = weaviate_key\n",
        "\n",
        "# try:\n",
        "#     f = open(Path.home() / \".weaviate.key\", \"r\")\n",
        "#     f.close()\n",
        "# except Exception as err:\n",
        "#     print(f\"Could not read your Weaviate key. Please make sure this is available in plain text under your home directory in ~/.weaviate.key: {err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d63ff4f-fdcb-48db-8c88-87b602fb0f86",
      "metadata": {
        "id": "7d63ff4f-fdcb-48db-8c88-87b602fb0f86"
      },
      "source": [
        "#### Preliminary config checks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: validate_rag_cfg from utils.rag_utils.py*\n",
        "def validate_rag_cfg(cfg):\n",
        "    if cfg[\"query_mode\"] == \"hybrid\":\n",
        "        assert (\n",
        "            cfg[\"hybrid_search_alpha\"] is not None\n",
        "        ), \"hybrid_search_alpha cannot be None if query_mode is set to 'hybrid'\"\n",
        "    if cfg[\"vector_db_type\"] == \"weaviate\":\n",
        "        assert (\n",
        "            cfg[\"weaviate_url\"] is not None\n",
        "        ), \"weaviate_url cannot be None for weaviate vector db\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "DczvCm8KJezw"
      },
      "id": "DczvCm8KJezw",
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "08e83dc8-9449-41d6-899d-0189bacaf437",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08e83dc8-9449-41d6-899d-0189bacaf437",
        "outputId": "d7a05a70-1121-4373-957a-d9f36e6e1c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chunk_overlap': 0,\n",
            " 'chunk_size': 256,\n",
            " 'do_sample': False,\n",
            " 'embed_model_name': 'text-embedding-005',\n",
            " 'embed_model_type': 'vertex',\n",
            " 'eval_llm_name': 'gemini-2.0-pro-exp-02-05',\n",
            " 'eval_llm_type': 'vertex',\n",
            " 'hybrid_search_alpha': 0.0,\n",
            " 'llm_name': 'gemini-2.0-flash-001',\n",
            " 'llm_type': 'vertex',\n",
            " 'max_new_tokens': 512,\n",
            " 'query_mode': 'default',\n",
            " 'rerank_top_k': 3,\n",
            " 'response_mode': 'compact',\n",
            " 'retriever_similarity_top_k': 5,\n",
            " 'retriever_type': 'vector_index',\n",
            " 'temperature': 0.0,\n",
            " 'top_k': 50,\n",
            " 'top_p': 1.0,\n",
            " 'use_reranker': False,\n",
            " 'vector_db_name': 'Pubmed_QA',\n",
            " 'vector_db_type': 'weaviate',\n",
            " 'weaviate_url': 'https://ds4tx7ttr3ciaui5obmowg.c0.us-east1.gcp.weaviate.cloud'}\n"
          ]
        }
      ],
      "source": [
        "validate_rag_cfg(rag_cfg)\n",
        "pprint(rag_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8672715b-12b7-424d-8410-859ff336a757",
      "metadata": {
        "id": "8672715b-12b7-424d-8410-859ff336a757"
      },
      "source": [
        "## STAGE 1 - Load dataset and documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe5fd2a-f974-4bb5-a678-7110aba1bc32",
      "metadata": {
        "id": "8fe5fd2a-f974-4bb5-a678-7110aba1bc32"
      },
      "source": [
        "#### 1. Load PubMed QA dataset\n",
        "PubMedQA ([github](https://github.com/pubmedqa/pubmedqa)) is a biomedical question answering dataset. Each instance consists of a question, a context (extracted from PubMed abstracts), a long answer and a yes/no/maybe answer. We make use of the test split of [this](https://huggingface.co/datasets/bigbio/pubmed_qa) huggingface dataset for this notebook.\n",
        "\n",
        "**The context for each instance is stored as a text file** (referred to as documents), to align the task as a standard RAG use-case."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: task_dataset.py*\n",
        "import os\n",
        "import json\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "class PubMedQATaskDataset(data.Dataset):\n",
        "    def __init__(self, name, all_folds=False, split=\"test\"):\n",
        "        self.name = name\n",
        "        subset_str = \"pubmed_qa_labeled_fold{fold_id}\"\n",
        "        folds = [0] if not all_folds else list(range(10))\n",
        "\n",
        "        bigbio_data = []\n",
        "        source_data = []\n",
        "        for fold_id in folds:\n",
        "            bb_data = load_dataset(\n",
        "                self.name,\n",
        "                f\"{subset_str.format(fold_id=fold_id)}_bigbio_qa\",\n",
        "                split=split,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            s_data = load_dataset(\n",
        "                self.name,\n",
        "                f\"{subset_str.format(fold_id=fold_id)}_source\",\n",
        "                split=split,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            bigbio_data.append(bb_data)\n",
        "            source_data.append(s_data)\n",
        "        bigbio_data = concatenate_datasets(bigbio_data)\n",
        "        source_data = concatenate_datasets(source_data)\n",
        "\n",
        "        keys_to_keep = [\"id\", \"question\", \"context\", \"answer\", \"LONG_ANSWER\"]\n",
        "        data_elms = []\n",
        "        for elm_idx in tqdm(range(len(bigbio_data)), desc=\"Preparing data\"):\n",
        "            data_elms.append({k: bigbio_data[elm_idx][k] for k in keys_to_keep[:4]})\n",
        "            data_elms[-1].update(\n",
        "                {keys_to_keep[-1].lower(): source_data[elm_idx][keys_to_keep[-1]]}\n",
        "            )\n",
        "\n",
        "        self.data = data_elms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def mock_knowledge_base(\n",
        "        self,\n",
        "        output_dir,\n",
        "        one_file_per_sample=False,\n",
        "        samples_per_file=500,\n",
        "        sep=\"\\n\",\n",
        "        jsonl=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Write PubMed contexts to a text file, newline seperated\n",
        "        \"\"\"\n",
        "        pubmed_kb_dir = os.path.join(output_dir, \"pubmed_doc\")\n",
        "        os.makedirs(pubmed_kb_dir, exist_ok=True)\n",
        "\n",
        "        file_ext = \"jsonl\" if jsonl else \"txt\"\n",
        "\n",
        "        if not one_file_per_sample:\n",
        "            context_str = \"\"\n",
        "            context_files = []\n",
        "            for idx in range(len(self.data)):\n",
        "                if (idx + 1) % samples_per_file == 0:\n",
        "                    context_files.append(context_str.rstrip(sep))\n",
        "                else:\n",
        "                    if jsonl:\n",
        "                        context_elm_str = json.dumps(\n",
        "                            {\n",
        "                                \"id\": self.data[idx][\"id\"],\n",
        "                                \"context\": self.data[idx][\"context\"],\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        context_elm_str = self.data[idx][\"context\"]\n",
        "                    context_str += f\"{context_elm_str}{sep}\"\n",
        "\n",
        "            for file_idx in range(len(context_files)):\n",
        "                filepath = os.path.join(pubmed_kb_dir, f\"context{file_idx}.{file_ext}\")\n",
        "                with open(filepath, \"w\") as f:\n",
        "                    f.write(context_files[file_idx])\n",
        "\n",
        "        else:\n",
        "            assert not jsonl, \"Does not support jsonl if one_file_per_sample is True\"\n",
        "            for idx in range(len(self.data)):\n",
        "                filepath = os.path.join(\n",
        "                    pubmed_kb_dir, f'{self.data[idx][\"id\"]}.{file_ext}'\n",
        "                )\n",
        "                with open(filepath, \"w\") as f:\n",
        "                    f.write(self.data[idx][\"context\"])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MONSnHCe7DsJ"
      },
      "id": "MONSnHCe7DsJ",
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "abd814ec-3084-4a5f-9fa9-8def165fe77d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abd814ec-3084-4a5f-9fa9-8def165fe77d",
        "outputId": "43a054b6-cae9-4dac-e643-d553f1451efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading PubMed QA data ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing data: 100%|██████████| 500/500 [00:00<00:00, 1468.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data size: 500\n"
          ]
        }
      ],
      "source": [
        "print('Loading PubMed QA data ...')\n",
        "pubmed_data = PubMedQATaskDataset('bigbio/pubmed_qa')\n",
        "print(f\"Loaded data size: {len(pubmed_data)}\")\n",
        "pubmed_data.mock_knowledge_base(output_dir='./data', one_file_per_sample=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: TODO: write knowledge base to GCS - to simulate loading knowledge base from object storage*"
      ],
      "metadata": {
        "id": "8W0xH_NVAHta"
      },
      "id": "8W0xH_NVAHta"
    },
    {
      "cell_type": "markdown",
      "id": "0139f09c-bf13-4d4a-9637-c7be7e165ad8",
      "metadata": {
        "id": "0139f09c-bf13-4d4a-9637-c7be7e165ad8"
      },
      "source": [
        "#### 2. Load documents\n",
        "All metadata is excluded by default. Set the *exclude_llm_metadata_keys* and *exclude_embed_metadata_keys* flags to *false* for including it. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents.html) and the *DocumentReader* class from *rag_utils.py* for further details."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: DocumentReader from utils.rag_utils.py*\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader\n",
        ")\n",
        "\n",
        "from llama_index.readers.json import JSONReader\n",
        "\n",
        "class DocumentReader:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dir,\n",
        "        exclude_llm_metadata_keys=True,\n",
        "        exclude_embed_metadata_keys=True,\n",
        "    ):\n",
        "        self.input_dir = input_dir\n",
        "        self._file_ext = os.path.splitext(os.listdir(input_dir)[0])[1]\n",
        "\n",
        "        self.exclude_llm_metadata_keys = exclude_llm_metadata_keys\n",
        "        self.exclude_embed_metadata_keys = exclude_embed_metadata_keys\n",
        "\n",
        "    def load_data(self):\n",
        "        docs = None\n",
        "        # Use reader based on file extension of documents\n",
        "        # Only support '.txt' files as of now\n",
        "        if self._file_ext == \".txt\":\n",
        "            reader = SimpleDirectoryReader(input_dir=self.input_dir)\n",
        "            docs = reader.load_data()\n",
        "        elif self._file_ext == \".jsonl\":\n",
        "            reader = JSONReader()\n",
        "            docs = []\n",
        "            for file in os.listdir(self.input_dir):\n",
        "                docs.extend(\n",
        "                    reader.load_data(os.path.join(self.input_dir, file), is_jsonl=True)\n",
        "                )\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Does not support {self._file_ext} file extension for document files.\"\n",
        "            )\n",
        "\n",
        "        # Can choose if metadata need to be included as input when passing the doc to LLM or embeddings:\n",
        "        # https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents.html\n",
        "        # Exclude metadata keys from embeddings or LLMs based on flag\n",
        "        if docs is not None:\n",
        "            all_metadata_keys = list(docs[0].metadata.keys())\n",
        "            if self.exclude_llm_metadata_keys:\n",
        "                for doc in docs:\n",
        "                    doc.excluded_llm_metadata_keys = all_metadata_keys\n",
        "            if self.exclude_embed_metadata_keys:\n",
        "                for doc in docs:\n",
        "                    doc.excluded_embed_metadata_keys = all_metadata_keys\n",
        "\n",
        "        return docs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xd8KZP8g8w6U"
      },
      "id": "xd8KZP8g8w6U",
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "a2b8ff42-45ee-4ad2-bf72-ccca100e1286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2b8ff42-45ee-4ad2-bf72-ccca100e1286",
        "outputId": "49e75756-bee2-43ec-a9db-d2429cebbddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents ...\n",
            "No. of documents loaded: 500\n"
          ]
        }
      ],
      "source": [
        "print('Loading documents ...')\n",
        "reader = DocumentReader(input_dir=\"./data/pubmed_doc\")\n",
        "docs = reader.load_data()\n",
        "print(f'No. of documents loaded: {len(docs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: TODO: load the knowledge base from GCS*"
      ],
      "metadata": {
        "id": "WFdv0ymAASmk"
      },
      "id": "WFdv0ymAASmk"
    },
    {
      "cell_type": "markdown",
      "id": "c6db76ff-1c8c-4ada-8c3d-30b2ef7bca30",
      "metadata": {
        "id": "c6db76ff-1c8c-4ada-8c3d-30b2ef7bca30"
      },
      "source": [
        "## STAGE 2 - Load node parser, embedding, LLM and set service context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e540f6-2522-4d69-89e6-3233f665c589",
      "metadata": {
        "id": "77e540f6-2522-4d69-89e6-3233f665c589"
      },
      "source": [
        "#### 1. Load node parser to split documents into smaller chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "acd11331-fe79-4100-bcd9-127838159e1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acd11331-fe79-4100-bcd9-127838159e1c",
        "outputId": "c3960ad1-14d1-47d3-be5b-369fa817fa02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading node parser ...\n"
          ]
        }
      ],
      "source": [
        "print('Loading node parser ...')\n",
        "node_parser = SentenceSplitter(chunk_size=rag_cfg['chunk_size'], chunk_overlap=rag_cfg['chunk_overlap'])\n",
        "nodes = node_parser.get_nodes_from_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018b8905-8a8e-4495-90f4-4e69811d9d19",
      "metadata": {
        "id": "018b8905-8a8e-4495-90f4-4e69811d9d19"
      },
      "source": [
        "#### 2. Load embedding model\n",
        "LlamaIndex supports embedding models from OpenAI, Cohere, HuggingFace, etc. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#custom-embedding-model) for building a custom embedding model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: RAGEmbedding from utils.rag_utils.py - update to support using Vertex AI Gemini Embeddings models*\n",
        "from llama_index.embeddings.vertex_endpoint import VertexEndpointEmbedding\n",
        "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.embeddings.text_embeddings_inference import TextEmbeddingsInference\n",
        "import google.auth\n",
        "\n",
        "credentials, project_id = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "credentials.refresh(auth_req)\n",
        "\n",
        "class RAGEmbedding:\n",
        "    \"\"\"\n",
        "    LlamaIndex supports embedding models from OpenAI, Cohere, HuggingFace, etc.\n",
        "    https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\n",
        "    We can also build out custom embedding model:\n",
        "    https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#custom-embedding-model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_type, model_name):\n",
        "        self.model_type = model_type\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def load_model(self, **kwargs):\n",
        "        print(f\"Loading {self.model_type} embedding model ...\")\n",
        "        if self.model_type == \"hf\":\n",
        "            # Using bge base HuggingFace embeddings, can choose others based on leaderboard:\n",
        "            # https://huggingface.co/spaces/mteb/leaderboard\n",
        "            model = HuggingFaceEmbedding(\n",
        "                model_name=self.model_name,\n",
        "                device=\"cuda\",\n",
        "                trust_remote_code=True,\n",
        "            )  # max_length does not have any effect?\n",
        "        elif self.model_type == \"vertex\":\n",
        "            model = GoogleGenAIEmbedding(\n",
        "                model_name=self.model_name,\n",
        "                vertexai_config={\n",
        "                  \"project\": PROJECT_ID,\n",
        "                  \"location\": REGION,\n",
        "                },\n",
        "                embed_batch_size=100,\n",
        "            )\n",
        "        elif self.model_type == \"vertex-endpoint\":\n",
        "            model = VertexEndpointEmbedding(\n",
        "                endpoint_id=kwargs[\"embed_model_endpoint_id\"],\n",
        "                project_id=PROJECT_ID,\n",
        "                location=REGION,\n",
        "                endpoint_kwargs={\n",
        "                    \"use_dedicated_endpoint\": kwargs[\"embed_model_use_dedicated_endpoint\"],\n",
        "                },\n",
        "            )  # max_length does not have any effect?\n",
        "        elif self.model_type == \"openai\":\n",
        "            # TODO - Add OpenAI embedding model\n",
        "            # embed_model = OpenAIEmbedding()\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "uQeTSmv6Azxa",
        "cellView": "form"
      },
      "id": "uQeTSmv6Azxa",
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "cae31175-725c-46f9-ae43-5dc80f604649",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cae31175-725c-46f9-ae43-5dc80f604649",
        "outputId": "274219d9-2153-4e81-987e-36c97983c907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vertex embedding model ...\n"
          ]
        }
      ],
      "source": [
        "embed_model = RAGEmbedding(model_type=rag_cfg['embed_model_type'], model_name=rag_cfg['embed_model_name']).load_model(**rag_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6446adf-983e-451c-b5dd-9a178e3b1af7",
      "metadata": {
        "id": "b6446adf-983e-451c-b5dd-9a178e3b1af7"
      },
      "source": [
        "#### 3. Load LLM for generation\n",
        "LlamaIndex supports LLMs from OpenAI, Cohere, HuggingFace, AI21, etc. Please refer to [this](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html#example-using-a-custom-llm-model-advanced) for loading a custom LLM model for generation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: RAGLLM from utils.hosting_utils.py - updated for Gemini on Vertex*\n",
        "\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "from google.genai.types import HarmCategory, HarmBlockThreshold\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "class RAGLLM:\n",
        "    \"\"\"\n",
        "    LlamaIndex supports OpenAI, Cohere, AI21 and HuggingFace LLMs\n",
        "    https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_type, llm_name, api_base=None, api_key=None):\n",
        "        self.llm_type = llm_type\n",
        "        self.llm_name = llm_name\n",
        "\n",
        "        self._api_base = api_base\n",
        "        self._api_key = api_key\n",
        "\n",
        "        self.local_model_path = \"/model-weights\"\n",
        "\n",
        "    def load_model(self, **kwargs):\n",
        "        print(f\"Configuring {self.llm_type} LLM model ...\")\n",
        "        gen_arg_keys = [\"temperature\", \"top_p\", \"top_k\", \"do_sample\"]\n",
        "        gen_kwargs = {k: v for k, v in kwargs.items() if k in gen_arg_keys}\n",
        "        if self.llm_type == \"local\":\n",
        "            # Using local HuggingFace LLM stored at /model-weights\n",
        "            llm = HuggingFaceLLM(\n",
        "                tokenizer_name=f\"{self.local_model_path}/{self.llm_name}\",\n",
        "                model_name=f\"{self.local_model_path}/{self.llm_name}\",\n",
        "                device_map=\"auto\",\n",
        "                context_window=4096,\n",
        "                max_new_tokens=kwargs[\"max_new_tokens\"],\n",
        "                generate_kwargs=gen_kwargs,\n",
        "                # model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
        "            )\n",
        "        # jkwng: add vertex support\n",
        "        elif self.llm_type in [\"vertex\"]:\n",
        "            safety_settings = {\n",
        "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
        "            }\n",
        "            llm = GoogleGenAI(\n",
        "                model=self.llm_name,\n",
        "                temperature=kwargs[\"temperature\"],\n",
        "                max_tokens=kwargs[\"max_new_tokens\"],\n",
        "                vertexai_config={\n",
        "                    \"project\": PROJECT_ID,\n",
        "                    \"location\": REGION,\n",
        "                },\n",
        "                safety_settings=safety_settings,\n",
        "            )\n",
        "        elif self.llm_type in [\"vertex-endpoint\"]:\n",
        "            ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "                PROJECT_ID, REGION, kwargs[\"llm_endpoint_id\"] # llm_name is the endpoint id\n",
        "            )\n",
        "            BASE_URL = (\n",
        "              f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "            )\n",
        "            try:\n",
        "                if kwargs[\"llm_use_dedicated_endpoint\"]:\n",
        "                    BASE_URL = f\"https://{kwargs['llm_dedicated_dns']}/v1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "            except NameError:\n",
        "                pass\n",
        "            llm = OpenAILike(\n",
        "                model=self.llm_name,\n",
        "                temperature=kwargs[\"temperature\"],\n",
        "                max_tokens=kwargs[\"max_new_tokens\"],\n",
        "                api_base=BASE_URL,\n",
        "                api_key=creds.token,\n",
        "                is_chat_model=True,\n",
        "                top_p=kwargs[\"top_p\"],\n",
        "                top_k=kwargs[\"top_k\"],\n",
        "            )\n",
        "        elif self.llm_type in [\"openai\", \"kscope\"]:\n",
        "            llm = OpenAILike(\n",
        "                model=self.llm_name,\n",
        "                api_base=self._api_base,\n",
        "                api_key=self._api_key,\n",
        "                is_chat_model=True,\n",
        "                temperature=kwargs[\"temperature\"],\n",
        "                max_tokens=kwargs[\"max_new_tokens\"],\n",
        "                top_p=kwargs[\"top_p\"],\n",
        "                top_k=kwargs[\"top_k\"],\n",
        "            )\n",
        "        return llm"
      ],
      "metadata": {
        "id": "pwqNy5v9DH9A"
      },
      "id": "pwqNy5v9DH9A",
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "d99126d7-48d6-4551-9c64-2044f7962ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d99126d7-48d6-4551-9c64-2044f7962ffd",
        "outputId": "edb31c5f-8268-41f1-dba7-6342ee7f077b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring vertex LLM model ...\n"
          ]
        }
      ],
      "source": [
        "llm = RAGLLM(\n",
        "    llm_type=rag_cfg['llm_type'],\n",
        "    llm_name=rag_cfg['llm_name'],\n",
        "    # api_base=GENERATOR_BASE_URL,\n",
        "    # api_key=OPENAI_API_KEY,\n",
        ").load_model(**rag_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb554bd-9df1-444c-b75a-373e117eee06",
      "metadata": {
        "id": "feb554bd-9df1-444c-b75a-373e117eee06"
      },
      "source": [
        "#### 4. Use ```Settings``` to set the node parser, embedding model, LLM, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "4c62a809-0558-4e29-ae0d-451259663f60",
      "metadata": {
        "id": "4c62a809-0558-4e29-ae0d-451259663f60"
      },
      "outputs": [],
      "source": [
        "Settings.text_splitter = node_parser\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dceb1e4-5ed6-47cb-987f-93f186387269",
      "metadata": {
        "id": "5dceb1e4-5ed6-47cb-987f-93f186387269"
      },
      "source": [
        "## STAGE 3 - Create index using the appropriate vector store\n",
        "All vector stores supported by LlamaIndex along with their available features are listed [here](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html).\n",
        "\n",
        "If you are using LangChain, the supported vector stores can be found [here](https://python.langchain.com/docs/modules/data_connection/vectorstores/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*jkwng: Llama Index + Vertex AI Vector Store integration seems to be broken and has the following things that do not work:*\n",
        "\n",
        "- *Batch Updates to a staging bucket gives an error, only Streaming Index works*\n",
        "- *Retrieval is broken - the API has changed but the library has not been updated*\n",
        "\n",
        "*For the purposes of the notebook - we will use Weaviate*"
      ],
      "metadata": {
        "id": "rgiGDroMCANj"
      },
      "id": "rgiGDroMCANj"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: RAGIndex from utils.storage_utils.py - modified to use Vertex Vector Store*\n",
        "import faiss\n",
        "import os\n",
        "import weaviate\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
        "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore\n",
        "\n",
        "# from .rag_utils import get_embed_model_dim\n",
        "def get_embed_model_dim(embed_model):\n",
        "    embed_out = embed_model.get_text_embedding(\"Dummy Text\")\n",
        "    return len(embed_out)\n",
        "\n",
        "class RAGIndex:\n",
        "    \"\"\"\n",
        "    Use storage context to set custom vector store\n",
        "    Available options: https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html\n",
        "    Use Chroma: https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo.html\n",
        "    LangChain vector stores: https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_type, db_name):\n",
        "        self.db_type = db_type\n",
        "        self.db_name = db_name\n",
        "        self._persist_dir = f\"./.{db_type}_index_store/\"\n",
        "\n",
        "    def create_index(self, docs, save=True, **kwargs):\n",
        "        # Only supports Weaviate as of now\n",
        "        if self.db_type == \"weaviate\":\n",
        "            # with open(Path.home() / \".weaviate.key\", \"r\") as f:\n",
        "            #     weaviate_api_key = f.read().rstrip(\"\\n\")\n",
        "            weaviate_client = weaviate.connect_to_wcs(\n",
        "                cluster_url=kwargs[\"weaviate_url\"],\n",
        "                auth_credentials=weaviate.auth.AuthApiKey(WEAVIATE_API_KEY),\n",
        "            )\n",
        "            vector_store = WeaviateVectorStore(\n",
        "                weaviate_client=weaviate_client,\n",
        "                index_name=self.db_name,\n",
        "            )\n",
        "        elif self.db_type == \"local\":\n",
        "            # Use FAISS vector database for local index\n",
        "            faiss_dim = get_embed_model_dim(kwargs[\"embed_model\"])\n",
        "            faiss_index = faiss.IndexFlatL2(faiss_dim)\n",
        "            vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "        # jkwng: added Vertex AI Vector Search support here\n",
        "        elif self.db_type == \"vertex\":\n",
        "          # check if storage bucket exists\n",
        "          bucket_names = [\n",
        "              bucket.name for bucket in storage.Client().list_buckets()\n",
        "          ]\n",
        "\n",
        "          dst_bucket = f\"jkwng-{self.db_name.replace('_', '-').lower()}\"\n",
        "\n",
        "          if dst_bucket not in bucket_names:\n",
        "              print(f\"Creating bucket {dst_bucket} ...\")\n",
        "              storage.Client().create_bucket(dst_bucket, location=REGION)\n",
        "              print(f\"Bucket {dst_bucket} created\")\n",
        "          else:\n",
        "              print(f\"Bucket {dst_bucket} exists\")\n",
        "\n",
        "          # check if index exists already in vertex\n",
        "          index_names = [\n",
        "              index.resource_name\n",
        "              for index in aiplatform.MatchingEngineIndex.list(\n",
        "                  filter=f\"display_name={self.db_name}\"\n",
        "              )\n",
        "          ]\n",
        "\n",
        "          # create the index if it doesn't exist\n",
        "          if len(index_names) == 0:\n",
        "              print(f\"Creating Vector Search index {self.db_name} ...\")\n",
        "              vs_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "                  display_name=self.db_name,\n",
        "                  dimensions=768,\n",
        "                  approximate_neighbors_count=100,\n",
        "                  distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
        "                  shard_size=\"SHARD_SIZE_SMALL\",\n",
        "                  index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} created with resource name {vs_index.resource_name}\"\n",
        "              )\n",
        "          else:\n",
        "              vs_index = aiplatform.MatchingEngineIndex(index_name=index_names[0])\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} exists with resource name {vs_index.resource_name}\"\n",
        "              )\n",
        "\n",
        "          # create an endpoint to serve the index\n",
        "          endpoint_names = [\n",
        "              endpoint.resource_name\n",
        "              for endpoint in aiplatform.MatchingEngineIndexEndpoint.list(\n",
        "                  filter=f\"display_name={self.db_name}\"\n",
        "              )\n",
        "          ]\n",
        "\n",
        "          if len(endpoint_names) == 0:\n",
        "              print(\n",
        "                  f\"Creating Vector Search index endpoint {self.db_name} ...\"\n",
        "              )\n",
        "              vs_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "                  display_name=self.db_name, public_endpoint_enabled=True\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index endpoint {vs_endpoint.display_name} created with resource name {vs_endpoint.resource_name}\"\n",
        "              )\n",
        "          else:\n",
        "              vs_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
        "                  index_endpoint_name=endpoint_names[0]\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index endpoint {vs_endpoint.display_name} exists with resource name {vs_endpoint.resource_name}\"\n",
        "              )\n",
        "\n",
        "          # check if endpoint exists\n",
        "          index_endpoints = [\n",
        "              (deployed_index.index_endpoint, deployed_index.deployed_index_id)\n",
        "              for deployed_index in vs_index.deployed_indexes\n",
        "          ]\n",
        "\n",
        "          if len(index_endpoints) == 0:\n",
        "              print(\n",
        "                  f\"Deploying Vector Search index {vs_index.display_name} at endpoint {vs_endpoint.display_name} ...\"\n",
        "              )\n",
        "              vs_deployed_index = vs_endpoint.deploy_index(\n",
        "                  index=vs_index,\n",
        "                  deployed_index_id=self.db_name,\n",
        "                  display_name=self.db_name,\n",
        "                  machine_type=\"e2-standard-2\",\n",
        "                  min_replica_count=1,\n",
        "                  max_replica_count=1,\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} is deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "              )\n",
        "          else:\n",
        "              vs_deployed_index = aiplatform.MatchingEngineIndexEndpoint(\n",
        "                  index_endpoint_name=index_endpoints[0][0]\n",
        "              )\n",
        "              print(\n",
        "                  f\"Vector Search index {vs_index.display_name} is already deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "              )\n",
        "\n",
        "          # setup storage\n",
        "          vector_store = VertexAIVectorStore(\n",
        "              project_id=PROJECT_ID,\n",
        "              region=REGION,\n",
        "              index_id=vs_index.resource_name,\n",
        "              endpoint_id=vs_endpoint.resource_name,\n",
        "              gcs_bucket_name=dst_bucket,\n",
        "          )\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Incorrect vector db type - {self.db_type}\")\n",
        "\n",
        "        if os.path.isdir(self._persist_dir):\n",
        "            # Load if index already saved\n",
        "            print(f\"Loading index from {self._persist_dir} ...\")\n",
        "            storage_context = StorageContext.from_defaults(\n",
        "                vector_store=vector_store,\n",
        "                persist_dir=self._persist_dir,\n",
        "            )\n",
        "            index = load_index_from_storage(storage_context)\n",
        "        else:\n",
        "            # Re-index\n",
        "            print(\"Creating new index ...\")\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            index = VectorStoreIndex.from_documents(\n",
        "                docs, storage_context=storage_context\n",
        "            )\n",
        "            if save:\n",
        "                os.makedirs(self._persist_dir, exist_ok=True)\n",
        "                index.storage_context.persist(persist_dir=self._persist_dir)\n",
        "\n",
        "        return index"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uNBIQ24qWYt9"
      },
      "id": "uNBIQ24qWYt9",
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "23824c74-c247-46ff-86fc-dcf1a8bd6fcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23824c74-c247-46ff-86fc-dcf1a8bd6fcd",
        "outputId": "ac76c4d3-cd8d-4ec2-bc52-b4cd09e20eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading index from ./.weaviate_index_store/ ...\n"
          ]
        }
      ],
      "source": [
        "index = RAGIndex(\n",
        "    db_type=rag_cfg['vector_db_type'],\n",
        "    db_name=rag_cfg['vector_db_name'],\n",
        ").create_index(docs, **rag_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b175a4a5-0a66-41b3-a848-850ce048bc6c",
      "metadata": {
        "id": "b175a4a5-0a66-41b3-a848-850ce048bc6c"
      },
      "source": [
        "## STAGE 4 - Build query engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608d105b-87ca-4bac-a1b0-254f73297b0d",
      "metadata": {
        "id": "608d105b-87ca-4bac-a1b0-254f73297b0d"
      },
      "source": [
        "Now build a query engine using *retriever* and *response_synthesizer*. LlamaIndex also supports different types of [retrievers](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html) and [response modes](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#configuring-the-response-mode) for various use-cases.\n",
        "\n",
        "[Weaviate hybrid search](https://weaviate.io/blog/hybrid-search-explained) explains how dense and sparse search is combined."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng - RAGQueryEngine from utils.rag_utils.py - add support for Vertex AI*\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "from llama_index.core import (\n",
        "    PromptTemplate,\n",
        "    get_response_synthesizer,\n",
        ")\n",
        "\n",
        "class RAGQueryEngine:\n",
        "    \"\"\"\n",
        "    https://docs.llamaindex.ai/en/stable/understanding/querying/querying.html\n",
        "    TODO - Check other args for RetrieverQueryEngine\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, retriever_type, vector_index):\n",
        "        self.retriever_type = retriever_type\n",
        "        self.index = vector_index\n",
        "        self.retriever = None\n",
        "        self.node_postprocessor = None\n",
        "        self.response_synthesizer = None\n",
        "\n",
        "    def create(self, similarity_top_k, response_mode, **kwargs):\n",
        "        self.set_retriever(similarity_top_k, **kwargs)\n",
        "        self.set_response_synthesizer(response_mode=response_mode)\n",
        "        if kwargs[\"use_reranker\"]:\n",
        "            self.set_node_postprocessors(rerank_top_k=kwargs[\"rerank_top_k\"])\n",
        "        query_engine = RetrieverQueryEngine(\n",
        "            retriever=self.retriever,\n",
        "            node_postprocessors=self.node_postprocessor,\n",
        "            response_synthesizer=self.response_synthesizer,\n",
        "        )\n",
        "        return query_engine\n",
        "\n",
        "    def set_retriever(self, similarity_top_k, **kwargs):\n",
        "        # Other retrievers can be used based on the type of index: List, Tree, Knowledge Graph, etc.\n",
        "        # https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers.html\n",
        "        # Find LlamaIndex equivalents for the following:\n",
        "        # Check MultiQueryRetriever from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
        "        # Check Contextual compression from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/\n",
        "        # Check Ensemble Retriever from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble\n",
        "        # Check self-query from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/self_query\n",
        "        # Check WebSearchRetriever from LangChain: https://python.langchain.com/docs/modules/data_connection/retrievers/web_research\n",
        "        if self.retriever_type == \"vector_index\":\n",
        "            self.retriever = VectorIndexRetriever(\n",
        "                index=self.index,\n",
        "                similarity_top_k=similarity_top_k,\n",
        "                vector_store_query_mode=kwargs[\"query_mode\"],\n",
        "                alpha=kwargs[\"hybrid_search_alpha\"],\n",
        "            )\n",
        "        elif self.retriever_type == \"bm25\":\n",
        "            self.retriever = BM25Retriever(\n",
        "                nodes=kwargs[\"nodes\"],\n",
        "                tokenizer=kwargs[\"tokenizer\"],\n",
        "                similarity_top_k=similarity_top_k,\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Incorrect retriever type - {self.retriever_type}\"\n",
        "            )\n",
        "\n",
        "    def set_node_postprocessors(self, rerank_top_k=2):\n",
        "        # Node postprocessor: Porcessing nodes after retrieval before passing to the LLM for generation\n",
        "        # Re-ranking step can be performed here!\n",
        "        # Nodes can be re-ordered to include more relevant ones at the top: https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder\n",
        "        # https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors.html\n",
        "\n",
        "        self.node_postprocessor = [LLMRerank(top_n=rerank_top_k)]\n",
        "\n",
        "    def set_response_synthesizer(self, response_mode):\n",
        "        # Other response modes: https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html#configuring-the-response-mode\n",
        "        qa_prompt_tmpl = (\n",
        "            \"Context information is below.\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"Given the context information and not prior knowledge, answer the query while providing an explanation. \"\n",
        "            \"If your answer is in favour of the query, end your response with 'yes' otherwise end your response with 'no'.\\n\"\n",
        "            \"Query: {query_str}\\n\"\n",
        "            \"Answer: \"\n",
        "        )\n",
        "        qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl)\n",
        "\n",
        "        self.response_synthesizer = get_response_synthesizer(\n",
        "            text_qa_template=qa_prompt_tmpl,\n",
        "            response_mode=response_mode,\n",
        "        )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tsIR8wKb0-E0"
      },
      "id": "tsIR8wKb0-E0",
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "ce527d9f-9f18-444a-ab77-b110a5277a1c",
      "metadata": {
        "id": "ce527d9f-9f18-444a-ab77-b110a5277a1c"
      },
      "outputs": [],
      "source": [
        "def set_query_engine_args(rag_cfg, docs):\n",
        "    query_engine_args = {\n",
        "        \"similarity_top_k\": rag_cfg['retriever_similarity_top_k'],\n",
        "        \"response_mode\": rag_cfg['response_mode'],\n",
        "        \"use_reranker\": False,\n",
        "    }\n",
        "\n",
        "    # jkwng: add that retriever type vector_index could be \"vertex\" too\n",
        "    # jkwng: note we don't actually use hybrid search for vertex ai vector search\n",
        "    if (rag_cfg[\"retriever_type\"] == \"vector_index\") and (rag_cfg[\"vector_db_type\"] == \"weaviate\"):\n",
        "        query_engine_args.update({\n",
        "            \"query_mode\": rag_cfg[\"query_mode\"],\n",
        "            \"hybrid_search_alpha\": rag_cfg[\"hybrid_search_alpha\"]\n",
        "        })\n",
        "    elif (rag_cfg[\"retriever_type\"] == \"vector_index\") and (rag_cfg[\"vector_db_type\"] == \"vertex\"):\n",
        "        query_engine_args.update({\n",
        "            # jkwng: only default mode works with VVS\n",
        "            \"query_mode\": \"default\",\n",
        "            \"hybrid_search_alpha\": rag_cfg[\"hybrid_search_alpha\"],\n",
        "        })\n",
        "    elif rag_cfg[\"retriever_type\"] == \"bm25\":\n",
        "        nodes = Settings.text_splitter.get_nodes_from_documents(docs)\n",
        "        tokenizer = Settings.embed_model._tokenizer\n",
        "        query_engine_args.update({\"nodes\": nodes, \"tokenizer\": tokenizer})\n",
        "\n",
        "    if rag_cfg[\"use_reranker\"]:\n",
        "        query_engine_args.update({\"use_reranker\": True, \"rerank_top_k\": rag_cfg[\"rerank_top_k\"]})\n",
        "\n",
        "    return query_engine_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "f47bc0bd-3d83-4dce-b488-38806eed654f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f47bc0bd-3d83-4dce-b488-38806eed654f",
        "outputId": "77535506-d7f8-405b-c145-eed74c23a6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 0.0,\n",
            " 'query_mode': 'default',\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': False}\n"
          ]
        }
      ],
      "source": [
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "5502edfd-75d0-4b48-b75d-46e6adf9447d",
      "metadata": {
        "id": "5502edfd-75d0-4b48-b75d-46e6adf9447d"
      },
      "outputs": [],
      "source": [
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index,\n",
        ").create(**query_engine_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e6df972-2515-4968-9ea9-e2604f6ab82a",
      "metadata": {
        "id": "9e6df972-2515-4968-9ea9-e2604f6ab82a"
      },
      "source": [
        "## STAGE 5 - Finally query the model !\n",
        "**Note:** We are using keyword based search or sparse search since *hybrid_search_alpha* is set to 0.0 by default."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffaa9ea9-a3c9-4205-b050-d347a9f425dd",
      "metadata": {
        "id": "ffaa9ea9-a3c9-4205-b050-d347a9f425dd"
      },
      "source": [
        "#### [TODO] Change seed to experiment with a different sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "725791c9-371b-43d5-9d5f-9de5d01e04f0",
      "metadata": {
        "id": "725791c9-371b-43d5-9d5f-9de5d01e04f0"
      },
      "outputs": [],
      "source": [
        "random.seed(237)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "bd7abee9-59f7-481b-a644-45c107d10686",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd7abee9-59f7-481b-a644-45c107d10686",
        "outputId": "59a18a6e-b10b-479e-aa38-5a06188eeaa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': ['no'],\n",
            " 'context': 'Human immunodeficiency virus (HIV)-infected patients have '\n",
            "            'generally been excluded from transplantation. Recent advances in '\n",
            "            'the management and prognosis of these patients suggest that this '\n",
            "            'policy should be reevaluated. To explore the current views of '\n",
            "            'U.S. transplant centers toward transplanting asymptomatic '\n",
            "            'HIV-infected patients with end-stage renal disease, a written '\n",
            "            'survey was mailed to the directors of transplantation at all 248 '\n",
            "            'renal transplant centers in the United States. All 148 responding '\n",
            "            'centers said they require HIV testing of prospective kidney '\n",
            "            'recipients, and 84% of these centers would not transplant an '\n",
            "            'individual who refuses HIV testing. The vast majority of '\n",
            "            'responding centers would not transplant a kidney from a cadaveric '\n",
            "            '(88%) or a living donor (91%) into an asymptomatic HIV-infected '\n",
            "            'patient who is otherwise a good candidate for transplantation. '\n",
            "            'Among the few centers that would consider transplanting an '\n",
            "            'HIV-infected patient, not a single center had performed such a '\n",
            "            'transplant in the year prior to the survey. Most centers fear '\n",
            "            'that transplantation in the face of HIV infection would be '\n",
            "            'harmful to the individual, and some believe that it would be a '\n",
            "            'waste of precious organs.',\n",
            " 'id': '9603166',\n",
            " 'long_answer': 'The great majority of U.S. renal transplant centers will not '\n",
            "                'transplant kidneys to HIV-infected patients with end-stage '\n",
            "                'renal disease, even if their infection is asymptomatic. '\n",
            "                'However, advances in the management of HIV infection and a '\n",
            "                'review of relevant ethical issues suggest that this approach '\n",
            "                'should be reconsidered.',\n",
            " 'question': 'Should all human immunodeficiency virus-infected patients with '\n",
            "             'end-stage renal disease be excluded from transplantation?'}\n"
          ]
        }
      ],
      "source": [
        "sample_idx = random.randint(0, len(pubmed_data)-1)\n",
        "sample_elm = pubmed_data[sample_idx]\n",
        "pprint(sample_elm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng: extract_yes_no from utils.rag_utils.py*\n",
        "import re\n",
        "\n",
        "def extract_yes_no(resp):\n",
        "    match_pat = r\"\\b(?:yes|no)\\b\"\n",
        "    match_txt = re.search(match_pat, resp, re.IGNORECASE)\n",
        "    if match_txt:\n",
        "        return match_txt.group(0)\n",
        "    return \"none\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "X7lMVucG3O4N"
      },
      "id": "X7lMVucG3O4N",
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "id": "8279db69-6525-401a-b39c-69f6b83cb0af",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8279db69-6525-401a-b39c-69f6b83cb0af",
        "outputId": "313faa4a-e21d-40ba-b355-a12cb99cf079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
            "\n",
            "RESPONSE:\n",
            "-------------------------\n",
            "Based on the provided text, the initial policy was to exclude HIV-infected patients from transplantation. However, the text suggests this policy should be reevaluated due to recent advances in managing and predicting the prognosis of these patients. The survey results indicate that most transplant centers in the US are still hesitant to transplant kidneys into asymptomatic HIV-infected patients, primarily due to concerns about harm to the patient and the potential waste of organs. However, the text also presents data on transplanting elderly kidneys into younger recipients, showing acceptable survival rates. This suggests that age, like HIV status, might be a factor that warrants reevaluation rather than automatic exclusion. Therefore, based on the information provided, a complete exclusion may not be warranted.\n",
            "\n",
            "no\n",
            "\n",
            "-------------------------\n",
            "\n",
            "YES/NO: no\n",
            "\n",
            "GT ANSWER: no\n",
            "\n",
            "GT LONG ANSWER:\n",
            "-------------------------\n",
            "The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "query = sample_elm['question']\n",
        "\n",
        "response = query_engine.query(query)\n",
        "\n",
        "delim = \"\".join([\"-\"]*25)\n",
        "print(f'QUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1808e94-337b-4e4b-9637-2b79746ddddc",
      "metadata": {
        "id": "a1808e94-337b-4e4b-9637-2b79746ddddc"
      },
      "source": [
        "#### [OPTIONAL] [Ragas](https://docs.ragas.io/en/latest/) evaluation\n",
        "Following are the commonly used metrics for evaluating a RAG workflow:\n",
        "* [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/): Measures the factual correctness of the generated answer based on the retrived context. Value lies between 0 and 1. **Evaluated using a LLM.**\n",
        "* [Answer Relevance](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/answer_relevance/): Measures how relevant the answer is to the given query. Value lies between 0 and 1. **Evaluated using a LLM.**\n",
        "* [Context Precision](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/): Precision of the retriever as measured using the retrieved and the ground truth context. Value lies between 0 and 1. LLM can be used for evaluation.\n",
        "* [Context Recall](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_recall/): Recall of the retriever as measured using the retrieved and the ground truth context. Value lies between 0 and 1. LLM can be used for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14877b8-e764-4e1b-b450-c29381567d44",
      "metadata": {
        "id": "c14877b8-e764-4e1b-b450-c29381567d44"
      },
      "source": [
        "Note: If you are planning to use **OpenAI models as evaluation LLMs**, store your OpenAI API key in ```~/.ragas_openai.env``` using the following format:\n",
        "\n",
        "```bash\n",
        "   export RAGAS_OPENAI_BASE_URL=\"https://api.openai.com/v1\"\n",
        "   export RAGAS_OPENAI_API_KEY=<openai_api_key>\n",
        "```\n",
        "\n",
        "Once done, **uncomment the next cell** to load these environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "4eb7ac22-3b60-465f-9226-f0db639ddf2a",
      "metadata": {
        "id": "4eb7ac22-3b60-465f-9226-f0db639ddf2a"
      },
      "outputs": [],
      "source": [
        "# from utils.load_secrets import load_env_file_ragas\n",
        "# load_env_file_ragas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *jkwng RagasEval from utils.rag_utils.py - update to include support for Vertex AI Gemini*\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "\n",
        "from ragas import EvaluationDataset, evaluate as ragas_evaluate\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper, LlamaIndexEmbeddingsWrapper\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    NonLLMContextPrecisionWithReference,\n",
        "    NonLLMContextRecall,\n",
        "    ResponseRelevancy,\n",
        ")\n",
        "\n",
        "RAGAS_METRIC_MAP = {\n",
        "    \"faithfulness\": Faithfulness(),\n",
        "    \"relevancy\": ResponseRelevancy(),\n",
        "    \"recall\": NonLLMContextRecall(),\n",
        "    \"precision\": NonLLMContextPrecisionWithReference(),\n",
        "}\n",
        "\n",
        "class RagasEval:\n",
        "    def __init__(\n",
        "        self, metrics, eval_llm_type, eval_llm_name, embed_model_type, embed_model_name, **kwargs\n",
        "    ):\n",
        "        self.eval_llm_type = eval_llm_type  # \"openai\", \"cohere\", \"local\", \"kscope\", \"vertex\"\n",
        "        self.eval_llm_name = eval_llm_name\n",
        "\n",
        "        self.temperature = kwargs.get(\"temperature\", 0.0)\n",
        "        self.max_tokens = kwargs.get(\"max_tokens\", 256)\n",
        "\n",
        "        self.embed_model_type = embed_model_type # \"openai\", \"vertex\", \"vertex-endpoint\"\n",
        "        self.embed_model_name = embed_model_name\n",
        "        self.embed_model_endpoint_id = kwargs.get(\"embed_model_endpoint_id\", None)\n",
        "        self.embed_model_use_dedicated_endpoint = kwargs.get(\"embed_model_use_dedicated_endpoint\", False)\n",
        "\n",
        "        self._prepare_embedding()\n",
        "        self._prepare_llm()\n",
        "\n",
        "        self.metrics = [RAGAS_METRIC_MAP[elm] for elm in metrics]\n",
        "\n",
        "    def _prepare_data(self, data):\n",
        "        return EvaluationDataset.from_list(data)\n",
        "\n",
        "    def _prepare_embedding(self):\n",
        "        model_kwargs = {\"device\": \"cuda\", \"trust_remote_code\": True}\n",
        "        encode_kwargs = {\n",
        "            \"normalize_embeddings\": True\n",
        "        }  # set True to compute cosine similarity\n",
        "\n",
        "        if self.embed_model_type == \"openai\":\n",
        "          self.eval_embedding = LangchainEmbeddingsWrapper(\n",
        "              HuggingFaceEmbeddings(\n",
        "                model_name=self.embed_model_name,\n",
        "                model_kwargs=model_kwargs,\n",
        "                encode_kwargs=encode_kwargs,\n",
        "              )\n",
        "          )\n",
        "        elif self.embed_model_type == \"vertex\":\n",
        "          self.eval_embedding = LangchainEmbeddingsWrapper(\n",
        "              VertexAIEmbeddings(\n",
        "                  model_name=self.embed_model_name,\n",
        "                  credentials=credentials,\n",
        "              )\n",
        "          )\n",
        "        elif self.embed_model_type == \"vertex-endpoint\":\n",
        "          self.eval_embedding = LlamaIndexEmbeddingsWrapper(\n",
        "              VertexEndpointEmbedding(\n",
        "                endpoint_id=self.embed_model_endpoint_id,\n",
        "                project_id=PROJECT_ID,\n",
        "                location=REGION,\n",
        "                endpoint_kwargs={\n",
        "                    \"use_dedicated_endpoint\": self.embed_model_use_dedicated_endpoint,\n",
        "                },\n",
        "              )\n",
        "          )\n",
        "\n",
        "    def _prepare_llm(self):\n",
        "        if self.eval_llm_type == \"local\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                HuggingFaceEndpoint(\n",
        "                    repo_id=f\"meta-llama/{self.eval_llm_name}\",\n",
        "                    temperautre=self.temperature,\n",
        "                    max_new_tokens=self.max_tokens,\n",
        "                    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"kscope\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatOpenAI(\n",
        "                    model=self.eval_llm_name,\n",
        "                    temperature=self.temperature,\n",
        "                    max_tokens=self.max_tokens,\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"openai\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatOpenAI(\n",
        "                    model=self.eval_llm_name,\n",
        "                    temperature=self.temperature,\n",
        "                    max_tokens=self.max_tokens,\n",
        "                    base_url=os.environ[\"RAGAS_OPENAI_BASE_URL\"],\n",
        "                    api_key=os.environ[\"RAGAS_OPENAI_API_KEY\"],\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"cohere\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatCohere(\n",
        "                    model=self.eval_llm_name,\n",
        "                )\n",
        "            )\n",
        "        elif self.eval_llm_type == \"vertex\":\n",
        "            self.eval_llm = LangchainLLMWrapper(\n",
        "                ChatVertexAI(\n",
        "                  model_name=self.eval_llm_name,\n",
        "                  temperature=self.temperature,\n",
        "                  max_tokens=self.max_tokens,\n",
        "              )\n",
        "            )\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        eval_data = self._prepare_data(data)\n",
        "        result = ragas_evaluate(\n",
        "            dataset=eval_data,\n",
        "            metrics=self.metrics,\n",
        "            llm=self.eval_llm,\n",
        "            embeddings=self.eval_embedding,\n",
        "        )\n",
        "        return result"
      ],
      "metadata": {
        "id": "EHIQCTgqQHdM"
      },
      "id": "EHIQCTgqQHdM",
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "id": "2bdb6372-9364-4bd3-9b81-bf403f0bc006",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bdb6372-9364-4bd3-9b81-bf403f0bc006",
        "outputId": "585d285a-ec72-47ec-c38e-bc4eaa6767ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'reference': 'The great majority of U.S. renal transplant centers will not '\n",
            "               'transplant kidneys to HIV-infected patients with end-stage '\n",
            "               'renal disease, even if their infection is asymptomatic. '\n",
            "               'However, advances in the management of HIV infection and a '\n",
            "               'review of relevant ethical issues suggest that this approach '\n",
            "               'should be reconsidered.',\n",
            "  'reference_contexts': ['Human immunodeficiency virus (HIV)-infected patients '\n",
            "                         'have generally been excluded from transplantation. '\n",
            "                         'Recent advances in the management and prognosis of '\n",
            "                         'these patients suggest that this policy should be '\n",
            "                         'reevaluated. To explore the current views of U.S. '\n",
            "                         'transplant centers toward transplanting asymptomatic '\n",
            "                         'HIV-infected patients with end-stage renal disease, '\n",
            "                         'a written survey was mailed to the directors of '\n",
            "                         'transplantation at all 248 renal transplant centers '\n",
            "                         'in the United States. All 148 responding centers '\n",
            "                         'said they require HIV testing of prospective kidney '\n",
            "                         'recipients, and 84% of these centers would not '\n",
            "                         'transplant an individual who refuses HIV testing. '\n",
            "                         'The vast majority of responding centers would not '\n",
            "                         'transplant a kidney from a cadaveric (88%) or a '\n",
            "                         'living donor (91%) into an asymptomatic HIV-infected '\n",
            "                         'patient who is otherwise a good candidate for '\n",
            "                         'transplantation. Among the few centers that would '\n",
            "                         'consider transplanting an HIV-infected patient, not '\n",
            "                         'a single center had performed such a transplant in '\n",
            "                         'the year prior to the survey. Most centers fear that '\n",
            "                         'transplantation in the face of HIV infection would '\n",
            "                         'be harmful to the individual, and some believe that '\n",
            "                         'it would be a waste of precious organs.'],\n",
            "  'response': 'Based on the provided text, the initial policy was to exclude '\n",
            "              'HIV-infected patients from transplantation. However, the text '\n",
            "              'suggests this policy should be reevaluated due to recent '\n",
            "              'advances in managing and predicting the prognosis of these '\n",
            "              'patients. The survey results indicate that most transplant '\n",
            "              'centers in the US are still hesitant to transplant kidneys into '\n",
            "              'asymptomatic HIV-infected patients, primarily due to concerns '\n",
            "              'about harm to the patient and the potential waste of organs. '\n",
            "              'However, the text also presents data on transplanting elderly '\n",
            "              'kidneys into younger recipients, showing acceptable survival '\n",
            "              'rates. This suggests that age, like HIV status, might be a '\n",
            "              'factor that warrants reevaluation rather than automatic '\n",
            "              'exclusion. Therefore, based on the information provided, a '\n",
            "              'complete exclusion may not be warranted.\\n'\n",
            "              '\\n'\n",
            "              'no\\n',\n",
            "  'retrieved_contexts': ['Human immunodeficiency virus (HIV)-infected patients '\n",
            "                         'have generally been excluded from transplantation. '\n",
            "                         'Recent advances in the management and prognosis of '\n",
            "                         'these patients suggest that this policy should be '\n",
            "                         'reevaluated. To explore the current views of U.S. '\n",
            "                         'transplant centers toward transplanting asymptomatic '\n",
            "                         'HIV-infected patients with end-stage renal disease, '\n",
            "                         'a written survey was mailed to the directors of '\n",
            "                         'transplantation at all 248 renal transplant centers '\n",
            "                         'in the United States. All 148 responding centers '\n",
            "                         'said they require HIV testing of prospective kidney '\n",
            "                         'recipients, and 84% of these centers would not '\n",
            "                         'transplant an individual who refuses HIV testing. '\n",
            "                         'The vast majority of responding centers would not '\n",
            "                         'transplant a kidney from a cadaveric (88%) or a '\n",
            "                         'living donor (91%) into an asymptomatic HIV-infected '\n",
            "                         'patient who is otherwise a good candidate for '\n",
            "                         'transplantation. Among the few centers that would '\n",
            "                         'consider transplanting an HIV-infected patient, not '\n",
            "                         'a single center had performed such a transplant in '\n",
            "                         'the year prior to the survey. Most centers fear that '\n",
            "                         'transplantation in the face of HIV infection would '\n",
            "                         'be harmful to the individual, and some believe that '\n",
            "                         'it would be a waste of precious organs.',\n",
            "                         'Human immunodeficiency virus (HIV)-infected patients '\n",
            "                         'have generally been excluded from transplantation. '\n",
            "                         'Recent advances in the management and prognosis of '\n",
            "                         'these patients suggest that this policy should be '\n",
            "                         'reevaluated. To explore the current views of U.S. '\n",
            "                         'transplant centers toward transplanting asymptomatic '\n",
            "                         'HIV-infected patients with end-stage renal disease, '\n",
            "                         'a written survey was mailed to the directors of '\n",
            "                         'transplantation at all 248 renal transplant centers '\n",
            "                         'in the United States. All 148 responding centers '\n",
            "                         'said they require HIV testing of prospective kidney '\n",
            "                         'recipients, and 84% of these centers would not '\n",
            "                         'transplant an individual who refuses HIV testing. '\n",
            "                         'The vast majority of responding centers would not '\n",
            "                         'transplant a kidney from a cadaveric (88%) or a '\n",
            "                         'living donor (91%) into an asymptomatic HIV-infected '\n",
            "                         'patient who is otherwise a good candidate for '\n",
            "                         'transplantation. Among the few centers that would '\n",
            "                         'consider transplanting an HIV-infected patient, not '\n",
            "                         'a single center had performed such a transplant in '\n",
            "                         'the year prior to the survey. Most centers fear that '\n",
            "                         'transplantation in the face of HIV infection would '\n",
            "                         'be harmful to the individual, and some believe that '\n",
            "                         'it would be a waste of precious organs.',\n",
            "                         '98.8%, 87.5%, and 69.5% for the patient (P=0.642), '\n",
            "                         '92.9%, 81.3%, and 64.2% vs. 93.9%, 76.4%, and 69.5% '\n",
            "                         'for the graft (P=0.980), and 94.4%, 92.6%, and 77.4% '\n",
            "                         'vs. 94.3%, 86.7%, and 84.4% for the graft with death '\n",
            "                         'censured (P=0.747), respectively. Creatininaemias at '\n",
            "                         '1, 5, and 10 years were 172, 175, and 210 vs. 139, '\n",
            "                         '134, and 155 (P<0.05).',\n",
            "                         '98.8%, 87.5%, and 69.5% for the patient (P=0.642), '\n",
            "                         '92.9%, 81.3%, and 64.2% vs. 93.9%, 76.4%, and 69.5% '\n",
            "                         'for the graft (P=0.980), and 94.4%, 92.6%, and 77.4% '\n",
            "                         'vs. 94.3%, 86.7%, and 84.4% for the graft with death '\n",
            "                         'censured (P=0.747), respectively. Creatininaemias at '\n",
            "                         '1, 5, and 10 years were 172, 175, and 210 vs. 139, '\n",
            "                         '134, and 155 (P<0.05).',\n",
            "                         'Kidneys from elderly donors tend to be implanted in '\n",
            "                         'recipients who are also elderly. We present the '\n",
            "                         'results obtained after 10 years of evolution on '\n",
            "                         'transplanting elderly kidneys into young recipients. '\n",
            "                         'Ninety-one consecutive transplants are studied, '\n",
            "                         'carried out in our center with kidneys from cadaver '\n",
            "                         'donors older than 60 years implanted in recipients '\n",
            "                         'younger than 60 years. The control group is made up '\n",
            "                         'of 91 transplants, matched with those from the study '\n",
            "                         'group, whose donor and recipient were younger than '\n",
            "                         '60 years. There were no differences between groups '\n",
            "                         'with regard to recipient age, sex, cause of death '\n",
            "                         'and renal function of the donor, hepatitis C and '\n",
            "                         'cytomegalovirus serologies, cold ischemia time, '\n",
            "                         'tubular necrosis, immediate diuresis, need for '\n",
            "                         'dialysis, human leukocyte antigen incompatibilities, '\n",
            "                         'hypersensitized patients, acute rejection, waiting '\n",
            "                         'time on dialysis, and days of admission. Survival in '\n",
            "                         'both groups at 1, 5, and 10 years was 97.6%, 87.2%, '\n",
            "                         'and 76.6% vs.'],\n",
            "  'user_input': 'Should all human immunodeficiency virus-infected patients '\n",
            "                'with end-stage renal disease be excluded from '\n",
            "                'transplantation?'}]\n"
          ]
        }
      ],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "pprint(eval_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "ed0582ac-b840-457d-86eb-4aa9d8fc43de",
      "metadata": {
        "id": "ed0582ac-b840-457d-86eb-4aa9d8fc43de"
      },
      "outputs": [],
      "source": [
        "eval_obj = RagasEval(\n",
        "    metrics=[\"faithfulness\", \"relevancy\", \"recall\", \"precision\"],\n",
        "    max_tokens=1024,\n",
        "    **rag_cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "65d26888-6b7b-4a41-9dac-9c242554a136",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "690bb6596d72478ca803082e2fc9e21c",
            "ad29fb5dcfff4b40b3ad3702990b8357",
            "1461b1390db64f6f807c4cd3eb61b1ca",
            "47bf48a3fa16467f91d6560935f4b1a8",
            "189add46e9e94eb9ba0c4e7e09cdd01f",
            "e9f98adfc0954184add55b865c19e8a7",
            "d27bee3d48a648f18747a1b663aecc66",
            "159c4bb49a4d4fb78e9f3c9e4389573a",
            "c41b8dba1755455391a16d8fe6d6677b",
            "69b41f5ef1df4104a0aadb13ed319049",
            "034f9eff611044b7829d0f92150f4543"
          ]
        },
        "id": "65d26888-6b7b-4a41-9dac-9c242554a136",
        "outputId": "f77f5bd9-5f06-474d-850b-ecab6b5b179a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "690bb6596d72478ca803082e2fc9e21c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': 0.9091, 'answer_relevancy': 0.7480, 'non_llm_context_recall': 1.0000, 'non_llm_context_precision_with_reference': 1.0000}\n"
          ]
        }
      ],
      "source": [
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe5bcc7-900b-4939-aa71-65385b5c765c",
      "metadata": {
        "id": "dfe5bcc7-900b-4939-aa71-65385b5c765c"
      },
      "source": [
        "### 5.1 - Dense Search\n",
        "Set *hybrid_search_alpha* to 1.0 for dense vector search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "131eaa05-ae98-481c-9967-cc6c74b9d383",
      "metadata": {
        "id": "131eaa05-ae98-481c-9967-cc6c74b9d383"
      },
      "outputs": [],
      "source": [
        "rag_cfg[\"hybrid_search_alpha\"] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "9fba7d70-0bd7-42b7-ba52-f1924e96b338",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fba7d70-0bd7-42b7-ba52-f1924e96b338",
        "outputId": "491cfdea-3149-44e3-b87e-9388856bed63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 1.0,\n",
            " 'query_mode': 'default',\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': False}\n",
            "\n",
            "\n",
            "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
            "\n",
            "RESPONSE:\n",
            "-------------------------\n",
            "Based on the provided text, the initial policy was to exclude HIV-infected patients from transplantation. However, the text suggests this policy should be reevaluated due to recent advances in managing and predicting the prognosis of these patients. The survey results indicate that most transplant centers in the U.S. would not transplant kidneys into asymptomatic HIV-infected patients, primarily due to concerns about harm to the patient and the potential waste of organs. However, the text also highlights that some centers would consider it. Therefore, the information suggests a need to reconsider the blanket exclusion, but it doesn't definitively say that *all* HIV-infected patients *should not* be excluded. The data leans towards caution and further evaluation.\n",
            "\n",
            "no\n",
            "\n",
            "-------------------------\n",
            "\n",
            "YES/NO: no\n",
            "\n",
            "GT ANSWER: no\n",
            "\n",
            "GT LONG ANSWER:\n",
            "-------------------------\n",
            "The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "# Recreate query engine\n",
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)\n",
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index\n",
        ").create(**query_engine_args)\n",
        "\n",
        "# Get response\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Print response\n",
        "print(f'\\n\\nQUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "893b4da5-2391-49ca-bbe6-ed7f2433faa5",
      "metadata": {
        "id": "893b4da5-2391-49ca-bbe6-ed7f2433faa5"
      },
      "source": [
        "#### [OPTIONAL] Ragas evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "id": "78b240d3-2b09-4c28-8c20-6d9be47dc963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "9e60040eb1ed4ead80ef737b487f3752",
            "ad6a7f70236c4ce7a515fee627cf3571",
            "b4ff6fb9881846d0ba4b3f9255a6050b",
            "cc7bf8b5c77c4154bdb2d763419a2fcf",
            "feeac8ab276040abb515c9f53884a681",
            "ba36890f090f478f96d983a01f7038a9",
            "fb80ccdb108b43569e1f209846897187",
            "d15f44e7eb2b42f1843a2e443dc8c2e2",
            "d7ecd69da66e4160a144572511d488d1",
            "45b393dde0a84880a54c7147c8aa058a",
            "bd9c60febc8a43d288dd3aa1619e8eb6"
          ]
        },
        "id": "78b240d3-2b09-4c28-8c20-6d9be47dc963",
        "outputId": "a7a8c02a-f912-4691-dadd-d209e284518a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e60040eb1ed4ead80ef737b487f3752"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': 0.9091, 'answer_relevancy': 0.7484, 'non_llm_context_recall': 1.0000, 'non_llm_context_precision_with_reference': 1.0000}\n"
          ]
        }
      ],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "\n",
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69911c71-1639-4ed8-86cb-85abbbc15467",
      "metadata": {
        "id": "69911c71-1639-4ed8-86cb-85abbbc15467"
      },
      "source": [
        "### 5.2 - Hybrid Search\n",
        "Set *hybrid_search_alpha* to 0.5 for hybrid search with equal weightage for dense and sparse (keyword-based) search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "6a526b44-5473-4850-9209-7f9b9446f5b3",
      "metadata": {
        "id": "6a526b44-5473-4850-9209-7f9b9446f5b3"
      },
      "outputs": [],
      "source": [
        "rag_cfg[\"hybrid_search_alpha\"] = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "id": "bf513126-01fe-4f6f-b18a-b6c5224f07e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf513126-01fe-4f6f-b18a-b6c5224f07e0",
        "outputId": "dc2f2b5b-0dd9-46d0-8dfb-73fa139bc193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 0.5,\n",
            " 'query_mode': 'default',\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': False}\n",
            "\n",
            "\n",
            "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
            "\n",
            "RESPONSE:\n",
            "-------------------------\n",
            "Based on the provided text, the initial policy was to exclude HIV-infected patients from transplantation. However, the text suggests this policy should be reevaluated due to recent advances in managing and predicting the prognosis of these patients. The survey results indicate that most transplant centers in the U.S. would not transplant kidneys into asymptomatic HIV-infected patients, primarily due to concerns about harm to the patient and the potential waste of organs. However, the text also highlights that some centers would consider it. Therefore, the information suggests a need to reconsider the blanket exclusion, but it doesn't definitively say that *all* HIV-infected patients *should not* be excluded. The data leans towards caution and further evaluation.\n",
            "\n",
            "no\n",
            "\n",
            "-------------------------\n",
            "\n",
            "YES/NO: no\n",
            "\n",
            "GT ANSWER: no\n",
            "\n",
            "GT LONG ANSWER:\n",
            "-------------------------\n",
            "The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "# Recreate query engine\n",
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)\n",
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index\n",
        ").create(**query_engine_args)\n",
        "\n",
        "# Get response\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Print response\n",
        "print(f'\\n\\nQUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028e7abc-1fbd-4d66-a724-dddba4733721",
      "metadata": {
        "id": "028e7abc-1fbd-4d66-a724-dddba4733721"
      },
      "source": [
        "#### [OPTIONAL] Ragas evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "id": "03197082-eb44-46d7-b982-816f8811560f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "a6f8d60ba40147008cd52381f09bfa50",
            "27d75d06e970476a809bdbcc1e158a55",
            "c7858feb68094a40a8e8fec1fe7eab51",
            "a99bb207d6824e9bbb6e04569bef63c9",
            "576ada209cd7478f93a247ac8bc6fd5e",
            "94383daed9594457bc983575bb00012d",
            "1896d649862048fd9fa74da0f2f471a4",
            "56fefcd93ca2430bb84ee7b8cf04b667",
            "852f079db36045e091e205b815e68d96",
            "6f1c3f95d3d24c7eba4fe91c143a3abd",
            "d66d50a41f0b465396db9b72ec75da05"
          ]
        },
        "id": "03197082-eb44-46d7-b982-816f8811560f",
        "outputId": "2711fb9f-63ba-4a9e-f362-f77b18ff8f4f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6f8d60ba40147008cd52381f09bfa50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-experimental. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "ERROR:ragas.executor:Exception raised in Job[0]: OutputParserException(Failed to parse NLIStatementOutput from completion {}. Got: 1 validation error for NLIStatementOutput\n",
            "statements\n",
            "  Field required [type=missing, input_value={}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': nan, 'answer_relevancy': 0.7460, 'non_llm_context_recall': 1.0000, 'non_llm_context_precision_with_reference': 1.0000}\n"
          ]
        }
      ],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "\n",
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ca6e00-2580-4d98-a832-3fd0f8094e1a",
      "metadata": {
        "id": "17ca6e00-2580-4d98-a832-3fd0f8094e1a"
      },
      "source": [
        "### 5.3 - Using Re-ranker\n",
        "Set *use_reranker* to *True* to re-rank the context after retrieving it from the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "acad8e06-e9a1-4d3c-b6e5-fa8bc6b1b8a6",
      "metadata": {
        "id": "acad8e06-e9a1-4d3c-b6e5-fa8bc6b1b8a6"
      },
      "outputs": [],
      "source": [
        "rag_cfg[\"use_reranker\"] = True\n",
        "rag_cfg[\"hybrid_search_alpha\"] = 1.0 # Using dense search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "id": "6c714b8f-45db-4ab6-9e28-7afa8c8a31ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c714b8f-45db-4ab6-9e28-7afa8c8a31ed",
        "outputId": "a29d0353-deed-4e84-c68f-30b14e0f3eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hybrid_search_alpha': 1.0,\n",
            " 'query_mode': 'default',\n",
            " 'rerank_top_k': 3,\n",
            " 'response_mode': 'compact',\n",
            " 'similarity_top_k': 5,\n",
            " 'use_reranker': True}\n",
            "\n",
            "\n",
            "QUERY: Should all human immunodeficiency virus-infected patients with end-stage renal disease be excluded from transplantation?\n",
            "\n",
            "RESPONSE:\n",
            "-------------------------\n",
            "The context suggests a reevaluation of the policy of excluding HIV-infected patients from transplantation due to recent advances in managing and improving the prognosis of these patients. However, the survey results indicate that the vast majority of transplant centers in the US are still against transplanting kidneys into asymptomatic HIV-infected patients. The primary reasons cited are concerns about potential harm to the patient and the belief that it would be a waste of organs.\n",
            "\n",
            "Based on this information, there is not enough evidence to definitively say that all HIV-infected patients with end-stage renal disease should be excluded from transplantation. The context suggests the policy should be reevaluated.\n",
            "\n",
            "no\n",
            "\n",
            "-------------------------\n",
            "\n",
            "YES/NO: no\n",
            "\n",
            "GT ANSWER: no\n",
            "\n",
            "GT LONG ANSWER:\n",
            "-------------------------\n",
            "The great majority of U.S. renal transplant centers will not transplant kidneys to HIV-infected patients with end-stage renal disease, even if their infection is asymptomatic. However, advances in the management of HIV infection and a review of relevant ethical issues suggest that this approach should be reconsidered.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "# Recreate query engine\n",
        "query_engine_args = set_query_engine_args(rag_cfg, docs)\n",
        "pprint(query_engine_args)\n",
        "query_engine = RAGQueryEngine(\n",
        "    retriever_type=rag_cfg['retriever_type'],\n",
        "    vector_index=index\n",
        ").create(**query_engine_args)\n",
        "\n",
        "# Get response\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Print response\n",
        "print(f'\\n\\nQUERY: {query}\\n')\n",
        "print(f'RESPONSE:\\n{delim}\\n{response.response}\\n{delim}\\n')\n",
        "print(f'YES/NO: {extract_yes_no(response.response)}\\n')\n",
        "print(f'GT ANSWER: {sample_elm[\"answer\"][0]}\\n')\n",
        "print(f'GT LONG ANSWER:\\n{delim}\\n{sample_elm[\"long_answer\"]}\\n{delim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6f4714-ee80-46ba-bf14-ec4762f70a08",
      "metadata": {
        "id": "2b6f4714-ee80-46ba-bf14-ec4762f70a08"
      },
      "source": [
        "#### [OPTIONAL] Ragas evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "8e43ba3a-6824-4246-8df9-3e9ce7efa5b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "362501fc060c40ceaa90f6701bcec8d5",
            "bbc4709a41bc4f95b845559e45bfb392",
            "e8db60cf69f147f4b8f56b0d238c6b78",
            "e64467a27f60497ba48234434192eea9",
            "5219d348d3914d6398765a36e7ee7354",
            "d8200a2e26b7473194d81f7a0d1e2de9",
            "ea4dabff4d9842a290d9920cf2d2346d",
            "c2059c96837e4a178de77754d45586d2",
            "adc56dc7919c4141be9802ebe7d3cb23",
            "317ae8ccdd8f4f9e9cec16e0a9ee93f8",
            "a8085f694b0348f1b35f50655365db64"
          ]
        },
        "id": "8e43ba3a-6824-4246-8df9-3e9ce7efa5b1",
        "outputId": "b243fb36-97e1-4ccb-e184-aa3dc1f2f18d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "362501fc060c40ceaa90f6701bcec8d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n",
            "ERROR:asyncio:Exception in callback PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()\n",
            "handle: <Handle PollerCompletionQueue._handle_events(<_UnixSelecto...e debug=False>)()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi\", line 147, in grpc._cython.cygrpc.PollerCompletionQueue._handle_events\n",
            "BlockingIOError: [Errno 11] Resource temporarily unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': 1.0000, 'answer_relevancy': 0.7754, 'non_llm_context_recall': 1.0000, 'non_llm_context_precision_with_reference': 1.0000}\n"
          ]
        }
      ],
      "source": [
        "retrieved_nodes = query_engine.retriever.retrieve(query)\n",
        "\n",
        "eval_data = [dict({\n",
        "    \"user_input\": query,\n",
        "    \"response\": response.response,\n",
        "    \"retrieved_contexts\": [node.text for node in retrieved_nodes],\n",
        "    \"reference\": sample_elm['long_answer'],\n",
        "    \"reference_contexts\": [sample_elm[\"context\"]],\n",
        "})]\n",
        "\n",
        "eval_result = eval_obj.evaluate(eval_data)\n",
        "pprint(eval_result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag_pubmed_qa",
      "language": "python",
      "name": "rag_pubmed_qa"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "name": "darmod_pubmed_qa_llamaindex_vertex_ai_gemini.ipynb",
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "690bb6596d72478ca803082e2fc9e21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad29fb5dcfff4b40b3ad3702990b8357",
              "IPY_MODEL_1461b1390db64f6f807c4cd3eb61b1ca",
              "IPY_MODEL_47bf48a3fa16467f91d6560935f4b1a8"
            ],
            "layout": "IPY_MODEL_189add46e9e94eb9ba0c4e7e09cdd01f"
          }
        },
        "ad29fb5dcfff4b40b3ad3702990b8357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9f98adfc0954184add55b865c19e8a7",
            "placeholder": "​",
            "style": "IPY_MODEL_d27bee3d48a648f18747a1b663aecc66",
            "value": "Evaluating: 100%"
          }
        },
        "1461b1390db64f6f807c4cd3eb61b1ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_159c4bb49a4d4fb78e9f3c9e4389573a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c41b8dba1755455391a16d8fe6d6677b",
            "value": 4
          }
        },
        "47bf48a3fa16467f91d6560935f4b1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69b41f5ef1df4104a0aadb13ed319049",
            "placeholder": "​",
            "style": "IPY_MODEL_034f9eff611044b7829d0f92150f4543",
            "value": " 4/4 [00:17&lt;00:00,  5.35s/it]"
          }
        },
        "189add46e9e94eb9ba0c4e7e09cdd01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9f98adfc0954184add55b865c19e8a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d27bee3d48a648f18747a1b663aecc66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "159c4bb49a4d4fb78e9f3c9e4389573a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c41b8dba1755455391a16d8fe6d6677b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69b41f5ef1df4104a0aadb13ed319049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "034f9eff611044b7829d0f92150f4543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e60040eb1ed4ead80ef737b487f3752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad6a7f70236c4ce7a515fee627cf3571",
              "IPY_MODEL_b4ff6fb9881846d0ba4b3f9255a6050b",
              "IPY_MODEL_cc7bf8b5c77c4154bdb2d763419a2fcf"
            ],
            "layout": "IPY_MODEL_feeac8ab276040abb515c9f53884a681"
          }
        },
        "ad6a7f70236c4ce7a515fee627cf3571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba36890f090f478f96d983a01f7038a9",
            "placeholder": "​",
            "style": "IPY_MODEL_fb80ccdb108b43569e1f209846897187",
            "value": "Evaluating: 100%"
          }
        },
        "b4ff6fb9881846d0ba4b3f9255a6050b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d15f44e7eb2b42f1843a2e443dc8c2e2",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7ecd69da66e4160a144572511d488d1",
            "value": 4
          }
        },
        "cc7bf8b5c77c4154bdb2d763419a2fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45b393dde0a84880a54c7147c8aa058a",
            "placeholder": "​",
            "style": "IPY_MODEL_bd9c60febc8a43d288dd3aa1619e8eb6",
            "value": " 4/4 [00:17&lt;00:00,  5.56s/it]"
          }
        },
        "feeac8ab276040abb515c9f53884a681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba36890f090f478f96d983a01f7038a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb80ccdb108b43569e1f209846897187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15f44e7eb2b42f1843a2e443dc8c2e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7ecd69da66e4160a144572511d488d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45b393dde0a84880a54c7147c8aa058a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd9c60febc8a43d288dd3aa1619e8eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6f8d60ba40147008cd52381f09bfa50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27d75d06e970476a809bdbcc1e158a55",
              "IPY_MODEL_c7858feb68094a40a8e8fec1fe7eab51",
              "IPY_MODEL_a99bb207d6824e9bbb6e04569bef63c9"
            ],
            "layout": "IPY_MODEL_576ada209cd7478f93a247ac8bc6fd5e"
          }
        },
        "27d75d06e970476a809bdbcc1e158a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94383daed9594457bc983575bb00012d",
            "placeholder": "​",
            "style": "IPY_MODEL_1896d649862048fd9fa74da0f2f471a4",
            "value": "Evaluating: 100%"
          }
        },
        "c7858feb68094a40a8e8fec1fe7eab51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56fefcd93ca2430bb84ee7b8cf04b667",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_852f079db36045e091e205b815e68d96",
            "value": 4
          }
        },
        "a99bb207d6824e9bbb6e04569bef63c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f1c3f95d3d24c7eba4fe91c143a3abd",
            "placeholder": "​",
            "style": "IPY_MODEL_d66d50a41f0b465396db9b72ec75da05",
            "value": " 4/4 [00:42&lt;00:00, 12.61s/it]"
          }
        },
        "576ada209cd7478f93a247ac8bc6fd5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94383daed9594457bc983575bb00012d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1896d649862048fd9fa74da0f2f471a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56fefcd93ca2430bb84ee7b8cf04b667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852f079db36045e091e205b815e68d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f1c3f95d3d24c7eba4fe91c143a3abd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d66d50a41f0b465396db9b72ec75da05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "362501fc060c40ceaa90f6701bcec8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbc4709a41bc4f95b845559e45bfb392",
              "IPY_MODEL_e8db60cf69f147f4b8f56b0d238c6b78",
              "IPY_MODEL_e64467a27f60497ba48234434192eea9"
            ],
            "layout": "IPY_MODEL_5219d348d3914d6398765a36e7ee7354"
          }
        },
        "bbc4709a41bc4f95b845559e45bfb392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8200a2e26b7473194d81f7a0d1e2de9",
            "placeholder": "​",
            "style": "IPY_MODEL_ea4dabff4d9842a290d9920cf2d2346d",
            "value": "Evaluating: 100%"
          }
        },
        "e8db60cf69f147f4b8f56b0d238c6b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2059c96837e4a178de77754d45586d2",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adc56dc7919c4141be9802ebe7d3cb23",
            "value": 4
          }
        },
        "e64467a27f60497ba48234434192eea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_317ae8ccdd8f4f9e9cec16e0a9ee93f8",
            "placeholder": "​",
            "style": "IPY_MODEL_a8085f694b0348f1b35f50655365db64",
            "value": " 4/4 [00:16&lt;00:00,  5.06s/it]"
          }
        },
        "5219d348d3914d6398765a36e7ee7354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8200a2e26b7473194d81f7a0d1e2de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea4dabff4d9842a290d9920cf2d2346d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2059c96837e4a178de77754d45586d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc56dc7919c4141be9802ebe7d3cb23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "317ae8ccdd8f4f9e9cec16e0a9ee93f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8085f694b0348f1b35f50655365db64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}